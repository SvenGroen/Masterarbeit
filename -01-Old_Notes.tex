\subsection{Tabular Data}
\label{ch:preliminaries-dataSynthesis-tabularData}

%intro
%tabular data is one of the most common ways to maintain massive databases \cite{esmaeilpour2022BidiscriminatorGANTabular} \cite{yoon2020VIMEExtendingSuccess}
%tabular data is most common form of structured data \cite{hernandez2022SyntheticDataGeneration}
%tabular data format is used to store information such as demoraphic information in medical and finance datasets \cite{yoon2020VIMEExtendingSuccess}
%heterogeneous tabular data are the most commobly used form of data and is essential for numerous applications \cite{borisov2022DeepNeuralNetworks}
%- tabular data is structured and usually presented a table with data points as rows and features as columns \cite{borisov2022DeepNeuralNetworks} \cite{yoon2020VIMEExtendingSuccess}


% Types of tabular data (Numerical, Categorical, etc.)
%tabular data is heterogeneous and contains a variety of attribute types (cont. and cat.) \cite{borisov2022DeepNeuralNetworks}
%and is very different to other data modalities like images, audio where only 1 feature is present \cite{borisov2022DeepNeuralNetworks}
%- Continuous \cite{lederrey2022DATGANIntegratingExperta}
%- Categorical 
%    - Binary
%    - Nominal (no order)
%    - Ordinal (order exists)
%\cite{lederrey2022DATGANIntegratingExperta}
%--> categorical are qualitative values not implying any numerical ordering and can take one out of limited set of values \cite{lane2003IntroductionStatistics}
%--> continuous are quantitative and "meassured in terms of numbers"
%- Dates/Timestamps \cite{hernandez2022SyntheticDataGeneration}
%- tabular data can be static (independent rows in a table) or dynamic (time series/multivariate time series) \cite{padhi2021TabularTransformersModeling}



% Challenges
% '- tabular data is not homogeneous which makes it challenging to work with for NNs \cite{borisov2022DeepNeuralNetworks}
% - correlations among features are weaker compared to homogeneous data (they have a spatial or semantic relationship) \cite{borisov2022DeepNeuralNetworks} \cite{yoon2020VIMEExtendingSuccess} % semantic relationship --> use embeddings to introduce relationship 

% data related pitfalls (noise, impreciseness, different attribute types and value ranges, missing values, privacy issues) \cite{borisov2022DeepNeuralNetworks}
% Mix of categorical and continouse data types \cite{li2021ImprovingGANInverse}

%Continouse columns usually follow complex distributions \cite{li2021ImprovingGANInverse}
%values in a dataset can be dependent on other variables \cite{lederrey2022DATGANIntegratingExperta} (also across multiple entries --> find example)
%high cardinality can lead to very sparse high-dimensional feature vectors and nonrobust models 

% biggest challenges when working with tabular data are: \cite{borisov2022DeepNeuralNetworks}
%     1. low-quality training data
%         - missing values
%         - outliers
%         - data errors/inconsistency
%         - expensive to collect
%         - class imbalanced \cite{li2021ImprovingGANInverse}
%     2. missing or complex irregular spatial dependencies
%         - no spatial correlation between variables
%         - dependencies between features are rather complex and irregular
%         - inductive bias not present --> cnns unsuited? %LOOKUP MEANING IN SOURCE
%     3. dependency on preprocessing
%         - performance depends strongly on preprocessing strategy \cite{gorishniy2022EmbeddingsNumericalFeatures}
%         - categorical preprocess especially challenge --> sparse feature matrix (\gls{oh}) or synthetic ordering of unordered values (label enc) \cite{borisov2022DeepNeuralNetworks}
%         - information loss during (//due to the conversions?) \cite{fitkov-norris2012EvaluatingImpactCategorical}
%     4. Importance of single feature
%         - images class change only if several pixel change, tabular data 1 cell entity is sufficient for prediciton flip

%HIER WEITERMACHEN
% \cite{borisov2022DeepNeuralNetworks} provides a taxonomy for data learning for tabular data with "data transformation methods" into "single dim encodings" and "multi dim encodings" 
% two dimensions:\cite{borisov2022DeepNeuralNetworks}
%     1. single dimensional encoding
%         - categorical encoding (ordinal, label, \gls{oh}, binary encoding, leave-one-out-encoding, hash bashed) %Explanation for each in the paper
%     2. multi-dimensional encoding 
%         - vime trains encoder learns a representations of cat and num features into homogeneous representation
%         - Supertml transforms tabular data into image format


%- proper encoding of inputs (categorical in this paper) ahs a significant influence on models performance \cite{norris2012EvaluatingImpactCategorical}
%different Datatypes require different preprocessing \cite{fan2020RelationalDataSynthesisa} meaningfully \cite{lederrey2022DATGANIntegratingExperta}
% %\cite{gorishniy2022EmbeddingsNumericalFeatures} shows that special encodings of numerical data influences the results of classification %vllt motivation? 
% "the embedding step has a substantial impact on the model effectiveness, and its proper design can be a game-changer in tabular DL"
% categorical:
% - ordinal encoding -> assign int to each category
% - \gls{oh} -> binary vector for each category




% Definition of synthetic data
% Types of synthetic data (Audio, Image, Text, Tabular)
% % usage of synthetic data
% - synthetic data can be use for testing and validation of applications \cite{gilad2021SynthesizingLinkedData} and machine learning models \cite{dahmen2019SynSysSyntheticData}
% - allows data sharing \cite{hernandez2022SyntheticDataGeneration} %while fulfilling regulatory and privacy constraints \cite{zhao2022CTABGANEnhancingTabular} (complies with GDPR \cite{zhao2022CTABGANEnhancingTabular})
% % - rebalance datasets with synthetic data if dataset is skewed \cite{zhao2022CTABGANEnhancingTabular}
% %- access to data still major bottleneck for researches of ml/dl-models \cite{fan2020RelationalDataSynthesisa}
% %- access is often restricted due to sensitivity of data (medical records) \cite{esteban2017RealvaluedMedicalTimea}
% % - can be used as additional training data \cite{kim2021OCTGANNeuralODEbased}

% - can be used to create test data for software applications because developers might:\cite{whiting2008CreatingRealisticScenariobased}
%     - real data may not be available or not visible for certain developers for security reasons \cite{whiting2008CreatingRealisticScenariobased}
%     - test data needs to fulfil certain requirements depending on the testing scenario and obtaining the needed characteristics from real data is time-consuming \cite{whiting2008CreatingRealisticScenariobased}


% % differences between real and synthetic data
% %- synthetic data is cheap to generate \cite{leminh2021AirGenGANbasedSynthetica}
% % - and can be combined with real data for training purposes \cite{leminh2021AirGenGANbasedSynthetica}

% % Challenges of synthetic data (privacy, scalability, availability, etc.)



- two competing objectives in generating synthetic data: \cite{little2021GenerativeAdversarialNetworksa}
    1. high data utility \cite{little2021GenerativeAdversarialNetworksa}
    2. low disclosure risk \cite{little2021GenerativeAdversarialNetworksa}



    - Process or data driven methods \cite{goncalves2020GenerationEvaluationSynthetic}
    - Process driven
        - Simulation models \cite{kowalczyk2022TaxonomyUseSynthetic}
    - Data driven
        - Data Augmentation \cite{kowalczyk2022TaxonomyUseSynthetic}
        - statistical distributions \cite{kowalczyk2022TaxonomyUseSynthetic}
        - ML/DL- based Approaches \cite{kowalczyk2022TaxonomyUseSynthetic}

        interdependencies between variables must be captures \cite{lederrey2022DATGANIntegratingExperta}
        danger of overfitting and generelize relationships between columns which only are present in training data but not in unseen data \cite{lederrey2022DATGANIntegratingExperta}




        \cite{zhao2022CTABGANEnhancingTabular} identifies four complex distributions that can occur when working with real world tabular data.
        underlying properties of tabular data: \cite{zhao2022CTABGANEnhancingTabular}
        - Single Gaussian variables:
            Single mode Gaussian distributions are very common.
            The distribution of real data is close to a single mode Gaussian distribution
        
        Mixed data type variables:\cite{zhao2022CTABGANEnhancingTabular}
            a variable can be a mix of these two types + missing values. The Mortgage variable from the Loan dataset is a good example of mixed variable
            According to the data description, a loan holder can either have no mortgage (0 value) or a mortgage (any positive value). 
            In appearance, this variable is not a categorical type due to the numeric nature of the data. 
            So all 4 SOTA algorithms treat this variable as continuous type without capturing the special meaning of the value zero. 
            Hence, all 4 algorithms generate a value around 0 instead of exact 0. And the negative values for Mortgage have no/wrong meaning in the real world
        
        Long tail distributions:\cite{zhao2022CTABGANEnhancingTabular}
            real world data can have long tail distributions where most of the occurrences happen near the initial value of the distribution, and rare cases towards the end
            Real data clearly has 99\% of occurrences happening at the start of the range, 
            but the distribution extends until around 25000. 
            In comparison none of the synthetic data generators is able to learn and imitate this behavior.
        
        Skewed multi-mode continuous variables:\cite{zhao2022CTABGANEnhancingTabular}
            The term multimode is extended from Variational Gaussian Mixtures (VGM)
            This is not a typical Gaussian distribution. There is an obvious peak
            with several other lower peaks
            This behavior is difficult to capture for the SOTA data generator
        #

AUTOENCODERS:

        - autoencoders are generalization of pca, that does not find a low dim hyperplane in which the data lies but learns a non-linear manifold.


        from \cite{kingma2013AutoEncodingVariationalBayes}
        
        Figure 1 from \cite{Bank2020Autoencoders}
        
        % what are autoencoders
        encoder transforms input into latent space which is reconstructed by decoder \cite{razghandi2022VariationalAutoencoderGenerativea}
        --> suffer from "lack of regularity in latent space \cite{razghandi2022VariationalAutoencoderGenerativea}
        
        % what are variational autoencoders
        use the KL divergence and encode a gaussian distribution in latent space \cite{razghandi2022VariationalAutoencoderGenerativea}
        % how do they work
        % mathematical formulation
        % Advantages / Problems / Challenges


GANS:
% what are GAN's
\cite{goodfellow2020GenerativeAdversarialNetworks}
- have been used very sucessfully in many different domains \cite{li2022TTSGANTransformerbasedTimeSeries}
 

% how do they work
\cite{zhao2022CTABGANEnhancingTabular}:
- trained via a zero-sum min-max game 
- the discriminator tries to maximize the objective, while the generator tries to minimize it.
- mentor (D) providing feedback to a student (G) on the quality of his work


\cite{li2022TTSGANTransformerbasedTimeSeries}
- 2 NN (gen and dis)
- gen inp: rand vec of specified dimension; gen out: same dimension, as similar to real training data
- dis: binary classifier, distiguish real and generated data
--> play zero sum game against each other
--> trz to each nash equilibrium


TRANSFORMER:
relies on multiple self-attention layers, surpasses other network architectures and shows properties of universal computation engine \cite{li2022TTSGANTransformerbasedTimeSeries}

have been very successfull on textual and visual data (//siehe quellen im paper) \cite{borisov2022DeepNeuralNetworks} and also been applied to tabular data \cite{padhi2021TabularTransformersModeling} \cite{gorishniy2022EmbeddingsNumericalFeatures}

% What are transformers
% How do they work
% In what context are they used (usually not for data synthesis)
% Advantages / Problems / Challenges


Diffusion:
https://www.arxiv-vanity.com/papers/2209.10948/

https://theaisummer.com/latent-variable-models/

https://matheo.uliege.be/bitstream/2268.2/15989/15/Master_thesis.pdf

https://towardsdatascience.com/understanding-diffusion-probabilistic-models-dpms-1940329d6048 

https://www.youtube.com/watch?v=HoKDTa5jHvg&t=951s


% What are diffusion probabilistic models
first paper \cite{sohl-dickstein2015DeepUnsupervisedLearning}
first famouse paper \cite{ho2020DenoisingDiffusionProbabilistic}
improvements> \cite{nichol2021ImprovedDenoisingDiffusion}
follow up: \cite{dhariwal2021DiffusionModelsBeat}

\cite{ho2022ClassifierFreeDiffusionGuidance}

\cite{rombach2022HighResolutionImageSynthesis}

% mathematical formulation
% How do they work
% In what context are they used (usually image synthesis)
% Advantages / Problems / Challenges


% for tabular data
\cite{zheng2022DiffusionModelsMissing} for missing data

\cite{kotelnikov2022TabDDPMModellingTabular} tabddpm
\cite{hoogeboom2021ArgmaxFlowsMultinomial} multinomial diffusion


evaluation:

% Bias and stability
% Domain Expertise
- there is no universal metric for data synthesis \cite{hernandez2022SyntheticDataGeneration}
- utility and information disclosure metric dimensions \cite{goncalves2020GenerationEvaluationSynthetic}
- structural similarity \cite{elemam2020SevenWaysEvaluate}

- dim reduction
file:///C:/Users/SvenG/Downloads/z04_master_thesis_brenninkmeijer%20(1).pdf

file:///C:/Users/SvenG/Downloads/Evaluation_synthetischer_Daten.pdf



%%

Like in \glspl{vae} (\Autoref{ch:preliminaries-variationalAutoencoders}), the model is trained by minimizing its negative log-likelihood (= maximizing the log-likelihood) $-log(p_\theta(x_0))$.
$p_\theta(x_0)$ depends on all timesteps before $x_0$ and is, therefore, not tractable in practice \cite{zbinden2022ImplementingExperimentingDiffusion}.
As a solution, the variational lower bound on the negative log-likelihood can be computed as illustrated in \Autoref{eqn:vlb}: %(ELBO)

\begin{equation}
  \label{eqn:vlb}
  \begin{align*}
    -log(p_\theta(x_0)) \geq -log(p_\theta(x_0)) - KL(q(x_{1:T}|x_0) \parallel p_\theta(x_{1:T}|x_0))
  \end{align*}
\end{equation}

The \gls{kl}-divergence (\Autoref{eqn:kl-divergence}) is a non-negative measure of dissimilarity between two probability distributions. 
The right-hand side is reduced by minimizing the \gls{kl}-divergence as the objective function in the given inequality, leading to a smaller or equal left-hand side. 
Consequently, minimizing the \gls{kl}-divergence can only decrease the negative log-likelihood term on the left-hand side of the inequality. 
This implies that the objective function provides a lower bound on the log-likelihood and any improvement in minimizing the \gls{kl}-divergence will ultimately lead to an enhancement in this lower bound.
As it is usually the case that a loss function that is minimized

Rewriting $KL(q(x_{1:T}|x_0) \parallel p_\theta(x_{1:T}|x_0))$ to $log(\frac{q(x_{1:T})|x_0}{p_\theta(x_{1:T}|x_0)})$ and some reformulations (see \Autoref{eqn:vlb2_appendix}) using bayes-rule results in \Autoref{eqn:vlb2} leads to the following inequation \cite{ho2020DenoisingDiffusionProbabilistic}:

\begin{equation}
  \label{eqn:vlb2}
  \begin{align*}
    -log(p_\theta(x_0)) \leq log(\frac{q(x_{1:T})|x_0)}{p_\theta(x_{0:T})})
  \end{align*}
\end{equation}

\Autoref{eqn:vlb2} is the variational lower bound that is minimized during training to optimize $-log(p_\theta(x_0))$.
With additional reformulations (see \Autoref{eqn:vlb3_appendix}), the loss function can be derived as \cite{ho2020DenoisingDiffusionProbabilistic}:


%%