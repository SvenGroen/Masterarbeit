\chapter{Results}
\label{ch:results}

\section{Reproduction and Verification of Results}
\label{ch:results-reproduction}

Given that this thesis draws heavily upon the research conducted by \cite{kotelnikov2022TabDDPMModellingTabular},
it was deemed absolutely necessary to first reproduce the original experiments and subsequently verify the authors findings,
prior to conducting any new experiments.
Fortunately, the publicly available code \cite{akim2023TabDDPMModellingTabular}, facilitated the replication of the experiments with relative ease.

The reproduction is limited to the adult dataset from \autoref{ch:methods-datasets} and to the machine learning efficacy computed with regards to a tuned CatBoost \cite{prokhorenkova2018CatBoostUnbiasedBoosting} model.

Firstly, a CatBoost model was tuned on the adult dataset, using the provided tuning script (tune\_evaluation\_model.py) [TODO: Drin lassen oder weg lassen?].
Next, for each sampling algorithm, a model was trained, according to the configuration files provided by the authors.
Each configuration file contains the parameters the authors found during hyperparameter tuning.
Hence, with the models respective pipeline.py script, the best found model from hyperparameter tuning could be trained and saved.
Finally, the trained CatBoost and trained sampling model were used in the evaluation script (eval\_seeds.py), which calculates and reports the results.

\begin{table}[h]
	\centering
	\begin{tabular}{l|c|c|r}
		\hline
		\textbf{Model} & \textbf{Reproduction} & \textbf{Original} & \textbf{Difference} \\ \hline
		Real           & 0.815                 & 0.815             & 0                   \\ \hline
		TVAE           & 0.780                 & 0.781             & -0.001              \\ \hline
		CTABGAN        & 0.775                 & 0.783             & -0.008              \\ \hline
		CTABGAN+       & 0.775                 & 0.772             & +0.003              \\ \hline
		SMOTE          & 0.791                 & 0.791             & 0                   \\ \hline
		TabDDPM        & 0.794                 & 0.795             & -0.001              \\ \hline
	\end{tabular}
	\caption[Reproduction of original Results]{Comparison of the CatBoost F1-score on synthetic datasets, created by different sampling models.
		F1-Scores of the reproduction experiments are compared against the results reported by the original authors \cite[Table 4, p. 8]{kotelnikov2022TabDDPMModellingTabular}.}
	\label{tab:reproduction}
\end{table}

\autoref{tab:reproduction} shows the computed F1-scores achieved by the CatBoost model when trained on different synthetic datasets generated by different sampling models.
The scores that could be reproduced are almost exactly the same as the scores reported by the original authors.
All differences are within the standard deviation reported by the authors, except for the CTABGAN-model.
It is unclear, why the CTABGAN-model score deviates in the reproduction experiment from the original score.
It is important to note, that minor modifications to the code or different python library versions have caused this alternation, which where required in order to train the model in the cloud environment, specified in \autoref{ch:methods-experimentalSetup}.

Therefore, the results as reported by the authors could be overall reproduced and verified.

\section{Experimental Results}
\label{ch:results-experimentalResults}

% QUALITATIVE vs QUANTITIVE

\subsection[]{Baseline Experiments}
\label{ch:Baseline}

After verification that the models are able to reproduce the machine-learning efficacy scores as reported,
their performance is additionally evaluated using the similarity evaluation as proposed in \autoref{ch:conceptualDesign-Evaluation}.
\autoref{tab:ml_baseline} shows the complete machine learning efficacy score results for the different sampling techniques:

\begin{table}[h]
	\centering
	\begin{tabular}{l|c|c|c}
		\hline
		\textbf{Model} & \textbf{Accurarcy} & \textbf{F1}    & \textbf{ROC-AUC} \\ \hline
		Real           & 0.874              & 0.815          & 0.928            \\ \hline
		TVAE           & 0.845              & 0.781          & 0.900            \\ \hline
		CTABGAN        & 0.850              & 0.775          & 0.900            \\ \hline
		CTABGAN+       & 0.855              & 0.775          & 0.907            \\ \hline
		SMOTE          & 0.858              & 0.791          & 0.910            \\ \hline
		TabDDPM        & \textbf{0.860}     & \textbf{0.794} & \textbf{0.913}   \\ \hline
	\end{tabular}
	\caption[Machine learning efficacy baseline]{Machine learning efficacy (CatBoost) baseline results.}
	\label{tab:ml_baseline}
\end{table}


In addition to the machine learning efficacy scores, the similarity scores of the TabSynDex metric are computed (\autoref{tab:sim_baseline}).

\begin{table}[h]
	\centering
	\begin{tabular}{lrrrrrr}
		\toprule
		\textbf{Model} & \textbf{Similarity Score} & \textbf{Basic} & \textbf{Correlation} & \textbf{ML}    & \textbf{Support} & \textbf{pMSE}  \\
		\midrule
		Real           & 0.960                     & 0.992          & 0.943                & 0.998          & 0.984            & 0.882          \\
		TVAE           & 0.658                     & 0.854          & 0.814                & 0.962          & 0.657            & 0.000          \\
		CTABGAN        & 0.741                     & 0.940          & 0.832                & 0.984          & \textbf{0.947}   & 0.000          \\
		CTABGAN+       & 0.750                     & 0.969          & 0.882                & 0.990          & 0.892            & 0.019          \\
		SMOTE          & 0.723                     & 0.953          & 0.865                & \textbf{0.992} & 0.804            & 0.000          \\
		TabDDPM        & \textbf{0.759}            & \textbf{0.973} & \textbf{0.919}       & \textbf{0.992} & 0.874            & \textbf{0.035} \\
		\bottomrule
	\end{tabular}
	\caption[Similarity baseline]{Similarity baseline results using the TabSynDex metrices. Similarity Score is the average of the other five scores. Best result is highlighted in bold}
	\label{tab:sim_baseline}
\end{table}

These baseline experiments show, that the Diffusion based synthesis approach outperforms other models not only in terms of machine learning efficacy, but also in terms of other metrics.
However, \autoref{tab:sim_baseline} shows that the CTABGAN models outperform TabDDPM in the Support coverage metric.
Additionally, it is worth mentioning that even though TabDDPM achieves the highest \gls{pmse} score, it is still extremely low and almost 0, which is the same for all other models.
The authors of \cite{chundawat2022UniversalMetricRobust} essentially confirm this observation.
During their experiments, the tested sampling techniques (various \gls{gan}-based approaches) also struggle to produce any synthetic data which achieves a \gls{pmse} score that is noticeably higher than 0.
\autoref{tab:sim_baseline} indicates, that this observation also holds for a diffusion based approach, which hyperparameters were tuned towards a machine learning efficacy score using a CatBoost model.

\subsection[]{Experiment 1: Adding Tabular Processing}
\label{ch:Experiment-1}

In the first set of experiments, the different tabular processing mechanism described in \autoref{ch:architecture-tabularProcessor-implementations} are evaluated.
For this, the tabular processing mechanisms have been added to the pipeline of TabDDPM, as described in the concept in \autoref{fig:Overall_changed}.
Consequently, the tuning (tune\_ddpm.py, see \autoref{ch:scripts}) of the diffusion model with the additional tabular processing was required.
The models hyperparameter have again been tuned towards the machine learning efficacy of a CatBoost model.

The results of the machine learning efficacy and TabSynDex metric results can be found in \autoref{tab:exp1-ml} and \autoref{tab:exp1-sim} respectively.
\begin{table}[h]
	\centering
	\begin{tabular}{lrrr}
		\toprule
		\textbf{Model} & \textbf{Accurarcy} & \textbf{F1}    & \textbf{ROC-AUC} \\
		\midrule
		Real           & 0.874              & 0.815          & 0.928            \\
		TabDDPM        & 0.860              & 0.794          & 0.913            \\
		TabDDPM-BGM    & \textbf{0.863}     & \textbf{0.798} & \textbf{0.916}   \\
		TabDDPM-FT     & 0.785              & 0.552          & 0.821            \\
		\bottomrule
	\end{tabular}
	\caption[Experiment1-ML-Efficacy]{CatBoost Machine learning efficacy scores for different tabular processing techniques.}
	\label{tab:exp1-ml}
\end{table}

\begin{table}[h]
	\centering
	\begin{tabular}{lrrrrrr}
		\toprule
		\textbf{Model} & \textbf{Similarity Score} & \textbf{Basic} & \textbf{Correlation} & \textbf{ML}    & \textbf{Support} & \textbf{pMSE}  \\
		\midrule
		Real           & 0.960                     & 0.992          & 0.943                & 0.998          & 0.984            & 0.882          \\
		TabDDPM        & \textbf{0.759}            & \textbf{0.973} & \textbf{0.919}       & 0.992          & 0.874            & \textbf{0.035} \\
		TabDDPM-BGM    & 0.742                     & 0.964          & 0.918                & \textbf{0.996} & 0.831            & 0.000          \\
		TabDDPM-FT     & 0.595                     & 0.495          & 0.648                & 0.869          & \textbf{0.963}   & 0.000          \\
		\bottomrule
	\end{tabular}
	\caption[Experiment1-Similarity]{TabSynDex evaluation metric scores for different tabular processing techniques.}
	\label{tab:exp1-sim}
\end{table}

Both evaluations show, that the additional \gls{bgm} tabular processing seems to increase the ML-efficacy scores.
All metrics in \autoref{tab:exp1-ml} are highest for the TabDDPM-BGM model and the ML efficacy score of TabSynDex (which makes use of different models)
is highest for TabDDPM-BGM as well, although only by a slight margin.
\autoref{tab:exp1-sim} indicates, that this increase seems to come at the cost of reduced performance in the other metrices, which are highest for the Basic-, Correlation- and pMSE-Score for the plain TabDDPM version.
TabDDPM-FT compares significantly worse than its counterparts in the Correlation and Basic similarity score.
Interestingly, TabDDPM-FT performance significantly better in the Support score than the other versions and approximately 13 percentage-points worse in terms of ML efficacy computed by the TabSynDex metric.
More details can be seen in the \autoref{tab:exp1-ml}, that shows that especially the F1 score from TabDDPM-FT is much worse than the F1 score of the other models.
Lastly, neither the \gls{bgm} nor the \gls{ft} tabular processing enable the diffusion model to produce synthetic data that is able to increase the \gls{pmse} score.


\subsection[]{Experiment 2: Similarity Hyperparameter optimization}
\label{ch:Experiment-2}

The second set of experiments are very similar to the first experiments.
Instead of tuning the models hyperparameters after the machine learning efficacy, as proposed by the original authors,
the models hyperparameters are tuned after the TabSynDex similarity score.

The results of the machine learning efficacy and TabSynDex metric results can be found in \autoref{tab:exp2-ml} and \autoref{tab:exp2-sim} respectively.

TODO FT simTune; ctabgan+simTUne

\begin{table}[h]
	\centering
	\begin{tabular}{lrrr}
		\toprule
		\textbf{Model} & \textbf{Accurarcy} & \textbf{F1}    & \textbf{ROC-AUC} \\
		\midrule
		Real           & 0.874              & 0.815          & 0.928            \\
		TabDDPM        & 0.856              & 0.782          & 0.908            \\
		TabDDPM-BGM    & \textbf{0.859}     & \textbf{0.792} & \textbf{0.911}   \\
		CTABGAN        & 0.850              & 0.776          & 0.900            \\
		TVAE           & 0.845              & 0.780          & 0.900            \\
		\bottomrule
	\end{tabular}
	\caption[Experiment2-ML-Efficacy]{CatBoost Machine learning efficacy scores for different tabular processing techniques which hyperparameter have been tuned towards the TabSynDex similarity score.}
	\label{tab:exp2-ml}
\end{table}

\begin{table}[h]
	\centering
	\begin{tabular}{lrrrrrr}
		\toprule
		\textbf{Model} & \textbf{Similarity Score} & \textbf{Basic} & \textbf{Correlation} & \textbf{ML}    & \textbf{Support} & \textbf{pMSE}  \\
		\midrule
		Real           & 0.960                     & 0.992          & 0.943                & 0.998          & 0.984            & 0.882          \\
		TabDDPM        & 0.852                     & 0.976          & \textbf{0.921}       & \textbf{0.991} & \textbf{0.952}   & 0.420          \\
		TabDDPM-BGM    & \textbf{0.857}            & \textbf{0.982} & 0.858                & \textbf{0.991} & 0.920            & \textbf{0.532} \\
		CTABGAN        & 0.740                     & 0.938          & 0.833                & 0.984          & 0.947            & 0.000          \\
		TVAE           & 0.658                     & 0.856          & 0.815                & 0.962          & 0.656            & 0.000          \\
		\bottomrule
	\end{tabular}
	\caption[Experiment2-Similarity]{TabSynDex evaluation metric scores for different tabular processing techniques which hyperparameter have been tuned towards the TabSynDex similarity score..}
	\label{tab:exp2-sim}
\end{table}


\begin{table}[h]
	\centering
	\caption{Comparison of Model A and Model B}
	\label{tab:model-comparison}
	\begin{tabular}{|l|ccc|ccc|}
		\toprule
		\multirow{2}{*}{\textbf{Metrics}} & \multicolumn{3}{c|}{\textbf{TabDDPM}} & \multicolumn{3}{c|}{\textbf{TabDDPM-BGM}}                                                                                      \\ \cline{2-7}
		                                  & \textbf{ML-Efficacy}                  & \textbf{Similarity Score}                 & \textbf{Diff.} & \textbf{ML-Efficacy} & \textbf{Similarity Score} & \textbf{Diff.} \\
		\midrule                          
		\multicolumn{1}{|l|}{Accurarcy}    & 0.82                                  & 0.73                                      & +0.            & 0.76                 & 0.81                      & +0.            \\
		\multicolumn{1}{|l|}{Metric 2}    & 0.69                                  & 0.64                                      & +0.            & 0.72                 & 0.71                      & +0.            \\
		\multicolumn{1}{|l|}{Metric 3}    & 0.91                                  & 0.88                                      & +0.            & 0.89                 & 0.86                      & +0.            \\
		\bottomrule
	\end{tabular}
\end{table}


The tuning of the hyperparameters had a significant effect on the performance of the different models.




\subsection[]{Comparisson}
\label{ch:Experiment-2}

% \begin{tabular}{lrrrrrr}
% 	\toprule
% 	method                      & acc    & f1     & roc_auc & sim_score & basic_score & corr_score & ml_score & sup_score & pmse_score \\
% 	\midrule
% 	real                        & 0.8742 & 0.8152 & 0.9276  & 0.9598    & 0.9922      & 0.9433     & 0.9975   & 0.9839    & 0.8820     \\
% 	tab-ddpm                    & 0.8598 & 0.7941 & 0.9128  & 0.7586    & 0.9730      & 0.9189     & 0.9923   & 0.8741    & 0.0349     \\
% 	tab-ddpm-bgm                & 0.8632 & 0.7985 & 0.9165  & 0.7418    & 0.9642      & 0.9183     & 0.9955   & 0.8307    & 0.0004     \\
% 	tab-ddpm-simTune            & 0.8556 & 0.7823 & 0.9078  & 0.8520    & 0.9764      & 0.9210     & 0.9910   & 0.9522    & 0.4196     \\
% 	tab-ddpm-bgm-simTune        & 0.8586 & 0.7917 & 0.9109  & 0.8567    & 0.9823      & 0.8579     & 0.9913   & 0.9197    & 0.5323     \\
% 	tab-ddpm-simTune-minmax     & 0.8561 & 0.7779 & 0.9100  & 0.8686    & 0.9375      & 0.9296     & 0.9901   & 0.9282    & 0.5575     \\
% 	tab-ddpm-bgm-simTune-minmax & 0.8568 & 0.7871 & 0.9088  & 0.8555    & 0.9812      & 0.9127     & 0.9921   & 0.9151    & 0.4763     \\
% 	tab-ddpm-ft                 & 0.7849 & 0.5516 & 0.8212  & 0.5951    & 0.4950      & 0.6482     & 0.8691   & 0.9633    & 0.0000     \\
% 	smote                       & 0.8582 & 0.7912 & 0.9104  & 0.7228    & 0.9528      & 0.8651     & 0.9925   & 0.8038    & 0.0000     \\
% 	ctabgan+                    & 0.8547 & 0.7747 & 0.9070  & 0.7503    & 0.9692      & 0.8818     & 0.9902   & 0.8915    & 0.0186     \\
% 	ctabgan                     & 0.8499 & 0.7750 & 0.8995  & 0.7406    & 0.9397      & 0.8321     & 0.9845   & 0.9468    & 0.0000     \\
% 	ctabgan_simTune             & 0.8500 & 0.7756 & 0.8999  & 0.7405    & 0.9385      & 0.8328     & 0.9841   & 0.9474    & 0.0000     \\
% 	tvae                        & 0.8450 & 0.7805 & 0.9003  & 0.6575    & 0.8544      & 0.8139     & 0.9620   & 0.6572    & 0.0000     \\
% 	tvae_simTune                & 0.8447 & 0.7805 & 0.9004  & 0.6577    & 0.8555      & 0.8146     & 0.9621   & 0.6563    & 0.0000     \\
% 	\bottomrule
% \end{tabular}






\subsection{Statistical Similarity}
\label{ch:results-experimentalResults-statisticalSimilarity}

\subsection{Machine Learning Efficiency}
\label{ch:results-experimentalResults-machineLearningEfficiency}

\subsection{Comparison of the Models}
\label{ch:results-experimentalResults-comparisonOfTheModels}
%-------------------------------------------------------------------------
\section{Analysis}
\label{ch:results-analysis}
%-------------------------------------------------------------------------


