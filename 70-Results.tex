\chapter{Results}
\label{ch:results}

\section{Reproduction and Verification of Results}
\label{ch:results-reproduction}

Given that this thesis draws heavily upon the research conducted by \cite{kotelnikov2022TabDDPMModellingTabular},
it was deemed absolutely necessary to first reproduce the original experiments and subsequently verify the authors findings,
prior to conducting any new experiments.
Fortunately, the publicly available code \cite{akim2023TabDDPMModellingTabular}, facilitated the replication of the experiments with relative ease.

The reproduction is limited to the adult dataset from \autoref{ch:methods-datasets} and to the machine learning efficacy computed with regards to a tuned CatBoost \cite{prokhorenkova2018CatBoostUnbiasedBoosting} model.

Firstly, a CatBoost model was tuned on the adult dataset, using the provided tuning script (tune\_evaluation\_model.py) [TODO: Drin lassen oder weg lassen?].
Next, for each sampling algorithm, a model was trained, according to the configuration files provided by the authors.
Each configuration file contains the parameters the authors found during hyperparameter tuning.
Hence, with the models respective pipeline.py script, the best found model from hyperparameter tuning could be trained and saved.
Finally, the trained CatBoost and trained sampling model were used in the evaluation script (eval\_seeds.py), which calculates and reports the results.

\begin{table}[h]
	\centering
	\begin{tabular}{l|c|c|r}
		\hline
		\textbf{Model} & \textbf{Reproduction} & \textbf{Original} & \textbf{Difference} \\ \hline
		Real           & 0.815                 & 0.815             & 0                   \\ \hline
		TVAE           & 0.780                 & 0.781             & -0.001              \\ \hline
		CTABGAN        & 0.775                 & 0.783             & -0.008              \\ \hline
		CTABGAN+       & 0.775                 & 0.772             & +0.003              \\ \hline
		SMOTE          & 0.791                 & 0.791             & 0                   \\ \hline
		TabDDPM        & 0.794                 & 0.795             & -0.001              \\ \hline
	\end{tabular}
	\caption[Reproduction of original Results]{Comparison of the CatBoost F1-score on synthetic datasets, created by different sampling models.
		F1-Scores of the reproduction experiments are compared against the results reported by the original authors \cite[Table 4, p. 8]{kotelnikov2022TabDDPMModellingTabular}.
	}
	\label{tab:reproduction}
\end{table}

\autoref{tab:reproduction} shows the computed F1-scores achieved by the CatBoost model when trained on different synthetic datasets generated by different sampling models.
The scores that could be reproduced are almost exactly the same as the scores reported by the original authors.
All differences are within the standard deviation reported by the authors, except for the CTABGAN-model.
It is unclear, why the CTABGAN-model score deviates in the reproduction experiment from the original score.
It is important to note, that minor modifications to the code or different python library versions have caused this alternation, which where required in order to train the model in the cloud environment, specified in \autoref{ch:methods-experimentalSetup}.

Therefore, the results as reported by the authors could be overall reproduced and verified.

\section{Experimental Results}
\label{ch:results-experimentalResults}

% QUALITATIVE vs QUANTITIVE

\subsection[]{Baseline Experiments}
\label{ch:Baseline}

After verification that the models are able to reproduce the machine-learning efficacy scores as reported,
their performance is additionally evaluated using the similarity evaluation as proposed in \autoref{ch:conceptualDesign-Evaluation}.
\autoref{tab:ml_baseline} shows the complete machine learning efficacy score results for the different sampling techniques:

\begin{table}[h]
	\centering
	\begin{tabular}{l|c|c|c}
		\hline
		\textbf{Model} & \textbf{Accurarcy} & \textbf{F1}    & \textbf{ROC-AUC} \\ \hline
		Real           & 0.874              & 0.815          & 0                \\ \hline
		TVAE           & 0.845              & 0.781          & 0.900            \\ \hline
		CTABGAN        & 0.850              & 0.775          & 0.900            \\ \hline
		CTABGAN+       & 0.855              & 0.775          & 0.907            \\ \hline
		SMOTE          & 0.858              & 0.791          & 0.910            \\ \hline
		TabDDPM        & \textbf{0.860}     & \textbf{0.794} & \textbf{0.913}   \\ \hline
	\end{tabular}
	\caption[Machine learning efficacy baseline]{Machine learning efficacy (CatBoost) baseline results.}
	\label{tab:ml_baseline}
\end{table}


In addition to the machine learning efficacy scores, the similarity scores of the TabSynDex metric are computed (\autoref{tab:sim_baseline}).

\begin{table}[h]
	\centering
	\begin{tabular}{lrrrrrr}
		\toprule
		\textbf{Model} & \textbf{Similarity Score} & \textbf{Basic} & \textbf{Correlation} & \textbf{ML}    & \textbf{Support} & \textbf{pMSE}   \\
		\midrule
		Real           & 0.960                     & 0.992          & 0.943                & 0.998          & 0.984            & 0.882           \\
		TVAE           & 0.658                     & 0.854          & 0.814                & 0.962          & 0.657            & 0.000           \\
		CTABGAN        & 0.741                     & 0.940          & 0.832                & 0.984          & \textbf{0.947}   & 0.000           \\
		CTABGAN+       & 0.750                     & 0.969          & 0.882                & 0.990          & 0.892            & 0.019           \\
		SMOTE          & 0.723                     & 0.953          & 0.865                & \textbf{0.992} & 0.804            & 0.000           \\
		TabDDPM        & \textbf{0.759}            & \textbf{0.973} & \textbf{0.919}       & \textbf{0.992} & 0.874            & \textbf{0.035}  \\
		\bottomrule
	\end{tabular}
	\caption[Similarity baseline]{Similarity baseline results using the TabSynDex metrices. Similarity Score is the average of the other five scores. Best result is highlighted in bold}
	\label{tab:sim_baseline}
\end{table}

These baseline experiments show, that the Diffusion based synthesis approach outperforms other models not only in terms of machine learning efficacy, but also in terms of other metrics.
However, \autoref{tab:sim_baseline} shows that the CTABGAN models outperform TabDDPM in the Support coverage metric.
Additionally, it is worth mentioning that even though TabDDPM achieves the highest \gls{pmse} score, it is still extremely low and almost 0, which is the same for all other models.
The authors of \cite{chundawat2022UniversalMetricRobust} essentially confirm this observation.
During their experiments, the tested sampling techniques (various \gls{gan}-based approaches) also struggle to produce any synthetic data which achieves a \gls{pmse} score that is noticeably higher than 0.
\autoref{tab:sim_baseline} indicates, that this observation also holds for a diffusion based approach, which hyperparameters were tuned towards a machine learning efficacy score using a CatBoost model.

\subsection[]{Experiment 1: Adding Tabular Processing}
\label{ch:Experiment-1}

In the first set of experiments, the different tabular processing mechanism described in \autoref{ch:architecture-tabularProcessor-implementations} are evaluated.
For this, the tabular processing mechanisms have been added to the pipeline of TabDDPM, as described in the concept in \autoref{fig:Overall_changed}.
Consequently, the tuning (tune\_ddpm.py, see \autoref{ch:scripts}) of the diffusion model with the additional tabular processing was required.
The models hyperparameter have again been tuned towards the machine learning efficacy of a CatBoost model.

The results of the machine learning efficacy and TabSynDex metric results can be found in \autoref{} and \autoref{} respectively.




% \begin{table}[h]
% 	\centering
% 	\begin{tabular}{lrrrrrr}
% 		\toprule
% 		\textbf{Model} & \textbf{\makecell[r]{Similarity \\ Score}} & \textbf{\makecell[r]{Basic \\ Score}} & \textbf{\makecell[r]{Correlation \\ Score}} & \textbf{\makecell[r]{ML \\ Score}} & \textbf{\makecell[r]{Support \\ Score}} & \textbf{\makecell[r]{pMSE \\ Score}} \\
%         \midrule
% 		Real            & 0.960                     & 0.992                & 0.943                      & 0.998             & 0.984                  & 0.882               \\
% 		TVAE            & 0.658                     & 0.854                & 0.814                      & 0.962             & 0.657                  & 0.000               \\
% 		CTABGAN         & 0.741                     & 0.940                & 0.832                      & 0.984             & \textbf{0.947}         & 0.000               \\
% 		CTABGAN+        & 0.750                     & 0.969                & 0.882                      & 0.990             & 0.892                  & 0.019               \\
% 		SMOTE           & 0.723                     & 0.953                & 0.865                      & \textbf{0.992}    & 0.804                  & 0.000               \\
% 		TabDDPM         & \textbf{0.759}            & \textbf{0.973}       & \textbf{0.919}             & \textbf{0.992}    & 0.874                  & \textbf{0.035 }     \\
% 		\bottomrule
% 	\end{tabular}
% 	\caption[Similarity baseline]{Similarity baseline results using the TabSynDex metrices. Similarity Score is the average of the other five scores. Best result is highlighted in bold}
% \end{table}










\begin{tabular}{lrrrrrr}
	\toprule
	method                      & acc    & f1     & roc_auc & sim_score & basic_score & corr_score & ml_score & sup_score & pmse_score \\
	\midrule
	real                        & 0.8742 & 0.8152 & 0.9276  & 0.9598    & 0.9922      & 0.9433     & 0.9975   & 0.9839    & 0.8820     \\
	tab-ddpm                    & 0.8598 & 0.7941 & 0.9128  & 0.7586    & 0.9730      & 0.9189     & 0.9923   & 0.8741    & 0.0349     \\
	tab-ddpm-bgm                & 0.8632 & 0.7985 & 0.9165  & 0.7418    & 0.9642      & 0.9183     & 0.9955   & 0.8307    & 0.0004     \\
	tab-ddpm-simTune            & 0.8556 & 0.7823 & 0.9078  & 0.8520    & 0.9764      & 0.9210     & 0.9910   & 0.9522    & 0.4196     \\
	tab-ddpm-bgm-simTune        & 0.8586 & 0.7917 & 0.9109  & 0.8567    & 0.9823      & 0.8579     & 0.9913   & 0.9197    & 0.5323     \\
	tab-ddpm-simTune-minmax     & 0.8561 & 0.7779 & 0.9100  & 0.8686    & 0.9375      & 0.9296     & 0.9901   & 0.9282    & 0.5575     \\
	tab-ddpm-bgm-simTune-minmax & 0.8568 & 0.7871 & 0.9088  & 0.8555    & 0.9812      & 0.9127     & 0.9921   & 0.9151    & 0.4763     \\
	tab-ddpm-ft                 & 0.7849 & 0.5516 & 0.8212  & 0.5951    & 0.4950      & 0.6482     & 0.8691   & 0.9633    & 0.0000     \\
	smote                       & 0.8582 & 0.7912 & 0.9104  & 0.7228    & 0.9528      & 0.8651     & 0.9925   & 0.8038    & 0.0000     \\
	ctabgan+                    & 0.8547 & 0.7747 & 0.9070  & 0.7503    & 0.9692      & 0.8818     & 0.9902   & 0.8915    & 0.0186     \\
	ctabgan                     & 0.8499 & 0.7750 & 0.8995  & 0.7406    & 0.9397      & 0.8321     & 0.9845   & 0.9468    & 0.0000     \\
	ctabgan_simTune             & 0.8500 & 0.7756 & 0.8999  & 0.7405    & 0.9385      & 0.8328     & 0.9841   & 0.9474    & 0.0000     \\
	tvae                        & 0.8450 & 0.7805 & 0.9003  & 0.6575    & 0.8544      & 0.8139     & 0.9620   & 0.6572    & 0.0000     \\
	tvae_simTune                & 0.8447 & 0.7805 & 0.9004  & 0.6577    & 0.8555      & 0.8146     & 0.9621   & 0.6563    & 0.0000     \\
	\bottomrule
\end{tabular}






\subsection{Statistical Similarity}
\label{ch:results-experimentalResults-statisticalSimilarity}

\subsection{Machine Learning Efficiency}
\label{ch:results-experimentalResults-machineLearningEfficiency}

\subsection{Comparison of the Models}
\label{ch:results-experimentalResults-comparisonOfTheModels}
%-------------------------------------------------------------------------
\section{Analysis}
\label{ch:results-analysis}
%-------------------------------------------------------------------------


