% Glossar Eintr√§ge:
\newglossaryentry{tst}{
    name=Test,
    description={TESTTESTTEST},
    plural=TESTS
}

\newglossaryentry{model}{
    name={Model},
    description={A model in machine learning or deep learning is a mathematical representation that captures the relationship between inputs and outputs in data.
    It is used to make predictions about new, unseen data by applying mathematical operations to the inputs.
    Deep learning models are a subset of parametric models in which the parameters are represented as weights in a neural network and are changed during training to reduce the discrepancy between expected and actual outcomes.
    Machine learning models include decision trees, logistic regression, random forests, and linear regression.
    Convolutional neural networks (CNNs), recurrent neural networks (RNNs), and autoencoders are a few examples of deep learning models.} \cite{parsons2021WhatMachineLearning},
    plural={models}
}

\newglossaryentry{lvm}{
    name={Latent variable model},
    description={
        Latent variable models are a type of probabilistic model that use latent or hidden variables to explain the observed data. 
        They model the relationship between the observed data and the latent variables through a set of conditional probability distributions.
        The goal is often to use this joint distribution to make predictions or perform inferences about the data, which typically involves marginalizing over the latent variables. 
        This means integrating over all possible values of the latent variables to obtain the probability distribution of the observed data. \cite{skrondal2007LatentVariableModelling}
        \cite{bishop1998latent}
    },
    plural={latent variable models}
}

\newglossaryentry{mc}{
    name={Markov chain},
    description={A Markov chain, also known as a Markov process, 
    represents a probabilistic model where the likelihood of an event occurring is determined 
    exclusively by the outcome of the preceding event in a series of potential occurrences \cite{gagniuc2017MarkovChainsTheory}
    },
    plural={Markov chains}
}

% Akkronyme
\newacronym{vgm}{VGM}{Variational Gaussian Mixture model}
\newacronym{cnn}{CNN}{Convolutional Neural Network}
\newacronym{rnn}{RNN}{Recurrent Neural Network}
\newacronym{lstm}{LSTM}{Long Short-Term Memory}
\newacronym{gru}{GRU}{Gated Recurrent Unit}
\newacronym{gdpr}{GDPR}{General Data Protection Regulation}
\newacronym{vae}{VAE}{Variational Autoencoders}
\newacronym{pca}{PCA}{Principal Component Analysis}
\newacronym{kl}{KL}{Kullback-Leibler}
\newacronym{gan}{GAN}{Generative Adversarial Network}
\newacronym{mlp}{MLP}{Multilayer Perceptron}
\newacronym{relu}{ReLU}{Rectified Linear Activation Unit}
\newacronym{ddpm}{DDPM}{Denoising Diffusion Probabilistic Model}
\newacronym{cdf}{CDF}{Cumulative Density Function}
\newacronym{mse}{MSE}{Mean-Squared Error}
\newacronym{pcd}{PCD}{Pairwise Correlation Difference}
\newacronym{dp}{DP}{Differential Privacy}
\newacronym{pmse}{pMSE}{Propensity Mean Squared Error}
\newacronym{rmse}{RMSE}{Root Mean Squared Error}
\newacronym{gmm}{GMM}{Gaussian Mixture Model}
\newacronym{nlp}{NLP}{Natural Language Processing}
\newacronym{gpu}{GPU}{Graphics Processing Unit}
\newacronym{bgm}{BGM}{Bayesian Gaussian Mixture Model}
\newacronym{dcr}{DCR}{Distance to Closest Record}
\newacronym{cpu}{CPU}{Central Processing Unit}
\newacronym{ft}{FT}{Feature Tokenization}
\newacronym{rq}{RQ}{Research Question}