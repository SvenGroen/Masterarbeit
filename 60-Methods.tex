\chapter{Methodology}
\label{ch:methodology}frac


\section*{Architecture}
\label{ch:architecture}

The architecture of the developed software was developed with the requirements in \autoref{ch:requirements} in mind.
The following section will explain how the individual additions to the existing code are designed and why the design was chosen.

\subsection[]{Tabular Processor Architecture}
\label{ch:architecture-tabularProcessor}

The tabular processor is defined as an abstract class with three core methods, each tabular processing mechanism needs to implement.
These functions are the fit, transform and inverse transform functions (FR[TODO]- FR[TODO]).
Thanks to the use of the abstract class, a clear definition of what a tabular transform should do is given.
Each tabular processor is different and realizes the transformation of the data in a different way, however,
each tabular processor will be used by the program in the same way (fit according to the data, transform the data and reconstruct transformed data).
Hence, additional tabular processing mechanism can be easily added, by just implementing the abstract methods (FR[TODO]).
Furthermore, the tabular processing mechanism can be exchanged easily, since they share the same functions.
To instantiate a tabular processor, the data separated into categorical, numerical and target as numpy-arrays \cite{harris2020array} need to be provided.
The first step in using the tabular processor is fitting it according to the data.
Additional required information for fitting can be provided through meta-data in the form of a dictionary.
Note that this fit function might not be necessary, depending on the processing mechanism, however, it should be implemented anyways.
Only after fitting, the transform function can be called, transforming the data and returning the transformed data in the same format.
The inverse transformation works in the same way, but just reversing transformed data back into its original format.

\subsection[]{Tabular Processor Implementations}
\label{ch:architecture-tabularProcessor-implementations}

Three different tabular processor versions have been implemented.
The IdentityProcessor, does not do anything to the data.
It is used to do experiments without any tabular processing mechanism.

\subsubsection*{BGMProcessor}
\label{ch:BGMProcessor}

The BGMProcessor is the same processing mechanism that has been introduced by the CTABGAN+ model \cite{zhao2022CTABGANEnhancingTabular}.
Firstly, a mixed-type encoding is introduced, specifically designed to encode single columns that contain both, numerical and categorical data types.
For the continues values, the authors adapt the mode-specific normalization technique from \cite{xu2019ModelingTabularData} using a \gls{bgm} implementation \cite{BayesianGaussianMixture}, which is a \gls{vgm} (see \autoref{sec:dataNormalization} for details).
For the categorical values, the $\beta$ one-hot vector is extended the number of possible categorical values in the column.
If a categorical value should be encoded, the $\alpha$ value is set to 0 and the one-hot encoding indicating the specific category is set to 1.
Additionally, the authors allow missing values, which are treated as another categorical entry.
Furthermore, the authors introduce a "general transform" mechanism \cite[p. 7]{zhao2022CTABGANEnhancingTabular} that is supposed to be used for simple distributions and counters the problem of exploding dimensionality caused by a one-hot encoding \cite{zhao2022CTABGANEnhancingTabular}.
This general transformation is mapping of the value into a range of  [-1, 1], that is achieved through a shifted and scaled min-max normalization (see MinMaxScaler in \autoref{sec:dataNormalization}).
Mathematically, the encoded datapoint $x^t_i$ can be computed using the original datapoint $x_i$ in column $x$ through the following formula:
$$x^t_i=2* \frac{x_i-\min(x)}{\max(x)}-\min(x)-1$$ 
which can easily be reversed with:
$$X_i = (\max(x)-\min(x))*\frac{X^t_i+1}{2}+\min(x)$$
Categorical values are label encoded before applying the normalization \cite{zhao2022CTABGANEnhancingTabular}.
The authors observe, that this simplistic general transformation for continuos values only works well for simple distribution (\eg single-mode gaussian) and not for complex distributions, for which the mode-spefic normalization is preferred \cite{zhao2022CTABGANEnhancingTabular}.
For categorical values, the general transformation should only be applied if the number of categories a column can take is very high and would lead to very sparse vectors that make computation difficult \cite{zhao2022CTABGANEnhancingTabular}.
Lastly, the authors log-transform columns that suffer from very long-tails (see \autoref{sec: synthetic tabular data generation}), because \Glspl{bgm} seem to have difficulties to encode values towards the tail \cite{zhao2022CTABGANEnhancingTabular}.
The log-transformation of each value $\tau$ in the column will be compressed to $\tau^e$, given a lower bound $l$ using:
\begin{equation}
    \label{eqn:log-transform}
    \begin{align*}
        \tau^e = 
        \begin{cases}
            log(\tau) &\text{if} l>0\\
            log(\tau-l+\epsilon) &\text{if} l\leq0 \text{, where } \epsilon > 0
        \end{cases}
    \end{align*}
\end{equation}

This encoding strategy is realized in the BGMProcessor class, that uses the DataPrep and DataTransformer class, as developed by \cite{zhao2022CTABGANEnhancingTabular}.
These classes require additional information about the dataset, provided by the user, including what columns are categorical, categorical in a numeric format [TODO:Überprüfen], numerical, mixed (including what values are categorical) as well as what columns require a log-transformation and which a general transformation.
Inside the Tabular Processor, this is realized by extending the info.json (\autoref{lst:info}) with an additional entry, dataset\_config [TODO: Kursiv?], see \autoref{lst:info_extended} for an example.

\begin{lstlisting}[label={lst:info_extended},caption={Example extended data info file}]
    {
    "name": "Adult",
    [...]
    "val_size": 6513,
    "dataset_config": {
        "cat_columns": [ // categorical columns
            "workclass", 
            (...), 
            "income"
            ],
        "non_cat_columns": [], // categorical columns in numeric form
        "log_columns": [], // log-transformation
        "general_columns": [ //  general-transformation
            "age"
            ], 
        "mixed_columns": { // mixed-data-types
            "capital-loss": [0.0], // the "0.0" value is categorical           
            "capital-gain": [0.0]
            },
        "int_columns": [ // numerical columns 
            "age", 
            (...), 
            "hours-per-week"
        ],
        "problem_type": "binclass", // [binclass|multiclass|regression]
        "target_column": "income"
        }
    }
\end{lstlisting}

\subsubsection*{FTProcessor}
\label{ch:FTProcessor}



%-------------------------------------------------------------------------
\section{Experimental Setup}
\label{ch:methods-experimentalSetup}
%-------------------------------------------------------------------------
\section{Datasets}
\label{ch:methods-datasets}
%-------------------------------------------------------------------------
\section{Reproduction of existing results}

\section{Implementation Details}
\label{ch:methods-implementationDetails}

\subsection{Model Architecture}
\label{ch:methods-implementationDetails-modelArchitecture}

\subsection{Training}
\label{ch:methods-implementationDetails-training}

\subsubsection{Hyperparameters}
\label{ch:methods-implementationDetails-training-hyperparameters}

\subsubsection{Train loop and loss function}
\label{ch:methods-implementationDetails-training-trainLoopAndLossFunction}

\subsection{Sampling}
\label{ch:methods-implementationDetails-sampling}

\subsection{Evaluation}
\label{ch:methods-implementationDetails-evaluation}
%-------------------------------------------------------------------------
