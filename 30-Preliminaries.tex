\chapter{Preliminaries}
\label{ch:preliminaries}

%-------------------------------------------------------------------------
\section{Data Synthesis}
\label{ch:preliminaries-dataSynthesis}

\subsection{Synthetic Data}
\label{ch:preliminaries-dataSynthesis-syntheticData}

% Definition of synthetic data
% Types of synthetic data (Audio, Image, Text, Tabular)
% usage of synthetic data
- synthetic data can be use for testing and validation of applications \cite{gilad2021SynthesizingLinkedData} and machine learning models \cite{dahmen2019SynSysSyntheticData}
- allows data sharing \cite{hernandez2022SyntheticDataGeneration} while fulfilling regulatory and privacy constraints \cite{zhao2022CTABGANEnhancingTabular} (complies with GDPR \cite{zhao2022CTABGANEnhancingTabular})
- rebalance datasets with synthetic data if dataset is skewed \cite{zhao2022CTABGANEnhancingTabular}
- access to data still major bottleneck for researches of ml/dl-models \cite{fan2020RelationalDataSynthesisa}
- access is often restricted due to sensitivity of data (medical records) \cite{esteban2017RealvaluedMedicalTimea}
- can be used as additional training data \cite{kim2021OCTGANNeuralODEbased}

- can be used to create test data for software applications because developers might:\cite{whiting2008CreatingRealisticScenariobased}
    - real data may not be available or not visible for certain developers for security reasons \cite{whiting2008CreatingRealisticScenariobased}
    - test data needs to fulfil certain requirements depending on the testing scenario and obtaining the needed characteristics from real data is time-consuming \cite{whiting2008CreatingRealisticScenariobased}


% differences between real and synthetic data
- synthetic data is cheap to generate \cite{leminh2021AirGenGANbasedSynthetica}
- and can be combined with real data for training purposes \cite{leminh2021AirGenGANbasedSynthetica}

% Challenges of synthetic data (privacy, scalability, availability, etc.)

- two competing objectives in generating synthetic data: \cite{little2021GenerativeAdversarialNetworksa}
    1. high data utility \cite{little2021GenerativeAdversarialNetworksa}
    2. low disclosure risk \cite{little2021GenerativeAdversarialNetworksa}


\subsection{Tabular Data}
\label{ch:preliminaries-dataSynthesis-tabularData}

Tabular data is the most one of the most common forms of structured structured data \cite{hernandez2022SyntheticDataGeneration} used to store, classify and share information \cite{pilaluisa2022ContextualWordEmbeddings}.
A table is made up of individual cell entries stored in rows and columns where rows can be seen as individual data points and columns are the different features \cite{borisov2022DeepNeuralNetworks, yoon2020VIMEExtendingSuccess}
This format is the most common way to maintain massive databases \cite{esmaeilpour2022BidiscriminatorGANTabular, yoon2020VIMEExtendingSuccess} 
and is crucial for applications that store heterogeneous information such as demographics, medical or financial information \cite{borisov2022DeepNeuralNetworks, yoon2020VIMEExtendingSuccess}.
Tabular data consists of multiple attribute types, such as categorical or continuous data types\cite{borisov2022DeepNeuralNetworks}.
While continuous values are of quantitative nature and stored in a numerical format, categorical values are made up by one value out of a limited set of values \cite{lederrey2022DATGANIntegratingExperta, lane2003IntroductionStatistics}.
Categorical attribute types can be further subdivided into binary, only two possible values, nominal, at least three possible values that do not follow any order and lastly ordinal with at least three entries where the values follow have an underlying order \cite{lederrey2022DATGANIntegratingExperta}.
This work adapts the formal definition of a table from \cite{xu2019ModelingTabularData}:

\begin{displayquote}
A table $T$ contains $N_{con}$ continuous columns and $N_{cat}$ categorical columns. [TODO: Definition ohne Discrete values]
\end{displayquote}

It is also possible that tabular data can contain other special data types like dates or timestamps which often contain information about the specific time a datapoint was recorded \cite{hernandez2022SyntheticDataGeneration}.
This kind of tabular data can be considered as dynamic tabular data, where individual records, \ie rows, can be dependent on each other, also known as a multivariate time series \cite{padhi2021TabularTransformersModeling}.
In static tabular data on the other hand the individual rows are independent from each other \cite{padhi2021TabularTransformersModeling}.
Hence, the order of rows and columns does not carry any meaning \cite{somepalli2021SAINTImprovedNeural}.
While the order of rows and columns does not matter, the individual values in one cell may vary well depend on values of another cell \cite{lederrey2022DATGANIntegratingExperta}.
An example for such an interdependency could occur in a demographics table, where an individual's legal status may depend on their age, 
for instance, a person under 18 years old is considered a minor and has different legal rights and responsibilities compared to an adult.

The authors of \cite{borisov2022DeepNeuralNetworks} identify four possible challenges when working with tabular data in a learning context.
The first identified challenge is the low quality of the data. Typical quality issues include missing values, noise in the data, extreme data points, data inconsistencies, class imbalance or high dimensionalities after preprocessing \cite{borisov2022DeepNeuralNetworks}[CONFIRM 1].
Secondly the missing irregular spatial dependencies of tabular data. Other common data formats like images or audio are homogeneous, 
meaning that they consists of only 1 feature type \cite{borisov2022DeepNeuralNetworks}.
Since tabular data consists of multiple features, made up of a mixture of categorical and numerical values, it is a heterogeneous data format with data points as rows and features as columns \cite{borisov2022DeepNeuralNetworks}.
This makes is especially challenging for neural networks to work with since the correlations between the features is weaker because they often do not have any form of spatial or semantic relationship like image or text data has \cite{borisov2022DeepNeuralNetworks, yoon2020VIMEExtendingSuccess}.
The third challenge is about the dependency on preprocessing. \cite{borisov2022DeepNeuralNetworks} highlights the importance of a preprocessing and explicit feature construction step that is necessary when working with tabular data in a deep learning context.
This preprocessing step is crucial since it does not only strongly influences the performance of deep learning models \cite{gorishniy2022EmbeddingsNumericalFeatures} 
it also introducing new challenges. Depending on the preprocessing strategy (\autoref{sec:preprocessing}) it is possible to create a very sparse feature matrix, create a synthetic ordinal ordering of a nominal variable or lose some information during the conversion of the data \cite{borisov2022DeepNeuralNetworks}.
The last and fourth challenge concerns the importance of a single feature. In homogeneous data multiple features need to change in order to change the class of the data. 
For heterogeneous tabular data a small change in one feature variable can already alter the class of the row. 
\cite{borisov2022DeepNeuralNetworks} illustrates this with the example of an image, where multiple pixels (\ie features) need to change in a coordinated manner in order to change the content (\ie class) of the image.
For tabular data a single change in a cell can change the prediction of a predictive model \cite{borisov2022DeepNeuralNetworks}. 
Consider a model that has to predict whether an individuals income is higher or lower than US\$ 50.000 per year \cite{Dua:2019} based on demographic information of that individual.
Switching an individuals "education" value from "Preschool" to "Doctorate" would likely cause the prediction to change from "<=50K" to ">50K".




%intro
%tabular data is one of the most common ways to maintain massive databases \cite{esmaeilpour2022BidiscriminatorGANTabular} \cite{yoon2020VIMEExtendingSuccess}
%tabular data is most common form of structured data \cite{hernandez2022SyntheticDataGeneration}
%tabular data format is used to store information such as demoraphic information in medical and finance datasets \cite{yoon2020VIMEExtendingSuccess}
%heterogeneous tabular data are the most commobly used form of data and is essential for numerous applications \cite{borisov2022DeepNeuralNetworks}
%- tabular data is structured and usually presented a table with data points as rows and features as columns \cite{borisov2022DeepNeuralNetworks} \cite{yoon2020VIMEExtendingSuccess}


% Types of tabular data (Numerical, Categorical, etc.)
%tabular data is heterogeneous and contains a variety of attribute types (cont. and cat.) \cite{borisov2022DeepNeuralNetworks}
%and is very different to other data modalities like images, audio where only 1 feature is present \cite{borisov2022DeepNeuralNetworks}
%- Continuous \cite{lederrey2022DATGANIntegratingExperta}
%- Categorical 
%    - Binary
%    - Nominal (no order)
%    - Ordinal (order exists)
%\cite{lederrey2022DATGANIntegratingExperta}
%--> categorical are qualitative values not implying any numerical ordering and can take one out of limited set of values \cite{lane2003IntroductionStatistics}
%--> continuous are quantitative and "meassured in terms of numbers"
%- Dates/Timestamps \cite{hernandez2022SyntheticDataGeneration}
%- tabular data can be static (independent rows in a table) or dynamic (time series/multivariate time series) \cite{padhi2021TabularTransformersModeling}



% Challenges
% '- tabular data is not homogeneous which makes it challenging to work with for NNs \cite{borisov2022DeepNeuralNetworks}
% - correlations among features are weaker compared to homogeneous data (they have a spatial or semantic relationship) \cite{borisov2022DeepNeuralNetworks} \cite{yoon2020VIMEExtendingSuccess} % semantic relationship --> use embeddings to introduce relationship 

% data related pitfalls (noise, impreciseness, different attribute types and value ranges, missing values, privacy issues) \cite{borisov2022DeepNeuralNetworks}
% Mix of categorical and continouse data types \cite{li2021ImprovingGANInverse}

%Continouse columns usually follow complex distributions \cite{li2021ImprovingGANInverse}
%values in a dataset can be dependent on other variables \cite{lederrey2022DATGANIntegratingExperta} (also across multiple entries --> find example)
%high cardinality can lead to very sparse high-dimensional feature vectors and nonrobust models 

% biggest challenges when working with tabular data are: \cite{borisov2022DeepNeuralNetworks}
%     1. low-quality training data
%         - missing values
%         - outliers
%         - data errors/inconsistency
%         - expensive to collect
%         - class imbalanced \cite{li2021ImprovingGANInverse}
%     2. missing or complex irregular spatial dependencies
%         - no spatial correlation between variables
%         - dependencies between features are rather complex and irregular
%         - inductive bias not present --> cnns unsuited? %LOOKUP MEANING IN SOURCE
%     3. dependency on preprocessing
%         - performance depends strongly on preprocessing strategy \cite{gorishniy2022EmbeddingsNumericalFeatures}
%         - categorical preprocess especially challenge --> sparse feature matrix (one-hot) or synthetic ordering of unordered values (label enc) \cite{borisov2022DeepNeuralNetworks}
%         - information loss during (//due to the conversions?) \cite{fitkov-norris2012EvaluatingImpactCategorical}
%     4. Importance of single feature
%         - images class change only if several pixel change, tabular data 1 cell entity is sufficient for prediciton flip



\subsubsection{Tabular Data Preprocessing}
\label{sec:preprocessing}

Different data types can and should be processed into a meaningful format to be useful for deep learning models in different ways \cite{fan2020RelationalDataSynthesisa, lederrey2022DATGANIntegratingExperta}.
Data preprocessing itself consists of multiple different tasks: Data cleaning, data normalization, data transformation, data integration, missing value imputation and noise identification \cite{tritscher2020EvaluationPosthocXAIa}
While each of the preprocessing tasks is in itself important, 
\cite{fitkov-norris2012EvaluatingImpactCategorical} and \cite{gorishniy2022EmbeddingsNumericalFeatures} showed that a proper transformation of categorical and numerical entries respectively can have a significant influence on a deep learning models performance.
\cite{xu2019ModelingTabularData} showed the importance of normalization for synthetic tabular data generation.
Since the focus of the work is on tabular data and its synthesis, the following section will highlight the most important tabular data transformation and normalization approaches.

\subsubsubsection{Data Transformation}
\label{sec:dataTransformation}

\cite{borisov2022DeepNeuralNetworks} introduces a taxonomy for data transformation methods and subdivides the existing approaches into "Single-Dimensional Encodings" and "Multi-Dimensional Encodings".
The goal is to transform the different values column can take and transform them into a different (numeric) representation, such that it can be processed by a deep learning model.

\textbf{Single-Dimensional Encodings:}
Single-Dimensional encoding techniques encode each cell independently \cite{borisov2022DeepNeuralNetworks}.
The following approaches are common techniques to encode a categorical column entry, usually in a text format, into a numerical format.
In ordinal- (or label-) encoding a simple mapping from each category to a numeric value occurs. 
While this introduces a synthetic ordering of potentially unordered categories, one-hot encoding overcomes this issue by introducing a new vector with the length of all possible values a categorical column can take.
All values in this vector are assign to zero except one entry that represents the category that should be encoded, which is set to one.
However, this approach can lead to high dimensionality feature vectors if the cardinality of the unique categories in categorical columns is large.
Binary encoding tries to reduce the dimensionality by setting the vector length to a maximum of $log(c)$ for $c$ unique categorical values in a column.
Each possible value is mapped to a number like in ordinal-encoding starting at 0 but the number is represented as a binary vector.
The leave-one-out encoding technique is an approach to encode a categorical column based on the target column in a machine learning scenario. 
A categorical entry is replaced by the mean of the target variable of all rows where the same category is present, excluding the target value of the to be encoded value.
Lastly, a hash-based approach is worth mentioning, where a deterministic hash function transforms each category into a numerical form \cite{borisov2022DeepNeuralNetworks}.

Numerical data, such as integers or floating point numbers, can often be used directly in deep learning models without undergoing a special encoding process. 
This is because deep learning algorithms are designed to handle numerical data and can learn patterns and relationships within the data without the need for additional encoding.
However, \cite{gorishniy2022EmbeddingsNumericalFeatures} has shown that in some cases, encoding numerical data can improve the performance of deep learning models. 
Encoding numerical data can be achieved through various methods such as normalization (see \autoref{sec:dataNormalization}), discretization, or using embeddings.

Discretization techniques transform numerical features to categorical features. 
\cite{dougherty1995SupervisedUnsupervisedDiscretization} gives an overview on classical discretization techniques, such has equal interval width binning, where the continues values are divided and assign to certain amount of bins.
A modern approach by researchers from NVIDIA \cite{dong2022GeneratingSyntheticData} invented a tokenizer specifically for tabular data with float numbers. 
This tokenizer converts float numbers into a sequence of token IDs \cite{dong2022GeneratingSyntheticData}.

In Embedding techniques values that should be encoded (\eg words) are mapped to a vector representation. 
This vector of real numbers tries to capture "semantic regularities in vector spaces" \cite[p. 2]{pilaluisa2022ContextualWordEmbeddings}.
The goal of embeddings is to create a vector space, in which semantically similar values are also numerically similar \cite{pilaluisa2022ContextualWordEmbeddings}.
It can be differentiated between static embeddings and contextualized embeddings. 
While the former embedding technique always provides the same numerical representation for an input value, the latter embedding technique changes the vector representation of value based on its surrounding context \cite{pilaluisa2022ContextualWordEmbeddings}.
This is especially important in the natural language processing domain, where contextual embeddings has led to state-of-the-art improvements \cite{pilaluisa2022ContextualWordEmbeddings}.
Homonyms or polysemic words like "bat" or "second" are words that carry multiple different meanings and their semantic meaning therefore changes with the context they are used in. [TODO: QUELLE]
Hence, the contextualized embedding vector of the word "second" in the context of time (\eg "it took me 3 seconds") should be different to the one where "second" is used in the context of a competition (\eg "he achieved the second place").
Contextualized embeddings have to be learned during some form of (pre-) training \cite{devlin2019BERTPretrainingDeep, iida2021TABBIEPretrainedRepresentations, deng2021TURLTableUnderstanding}. 
Static embeddings, such as Word2Vec \cite{mikolov2013DistributedRepresentationsWords} are learned as well. 
It is also possible to use the embedding layers to get a vector representation without learning, which can be seen as a "feature tokenization" \cite{zheng2022DiffusionModelsMissing, gorishniy2021RevisitingDeepLearning}.
However, semantic similarity in a vector space cannot be achieved without any learning, so this embedding technique is more similar to a discretization/tokenization technique.


%HIER WEITERMACHEN
\cite{borisov2022DeepNeuralNetworks} provides a taxonomy for data learning for tabular data with "data transformation methods" into "single dim encodings" and "multi dim encodings" 
two dimensions:\cite{borisov2022DeepNeuralNetworks}
    1. single dimensional encoding
        - categorical encoding (ordinal, label, one-hot, binary encoding, leave-one-out-encoding, hash bashed) %Explanation for each in the paper
    2. multi-dimensional encoding 
        - vime trains encoder learns a representations of cat and num features into homogeneous representation
        - Supertml transforms tabular data into image format

\textbf{Multi-Dimensional Encodings:}
Multi-Dimensional encoding techniques focuses on encoding an entire record (or table-row) into another representation \cite{borisov2022DeepNeuralNetworks}.


\subsubsubsection{Data Normalization}
\label{sec:dataNormalization} 

numerical:
- min-max normalization
- GMM-based normalization


%- proper encoding of inputs (categorical in this paper) ahs a significant influence on models performance \cite{norris2012EvaluatingImpactCategorical}
%different Datatypes require different preprocessing \cite{fan2020RelationalDataSynthesisa} meaningfully \cite{lederrey2022DATGANIntegratingExperta}
%\cite{gorishniy2022EmbeddingsNumericalFeatures} shows that special encodings of numerical data influences the results of classification %vllt motivation? 
"the embedding step has a substantial impact on the model effectiveness, and its proper design can be a game-changer in tabular DL"
categorical:
- ordinal encoding -> assign int to each category
- one-hot -> binary vector for each category











\subsection{Synthetic Tabular Data Generation}

--> eacg data point as "row in table", or "sample from unknown joint distribution"
order of rows and columns does not carry any meaning \cite{somepalli2021SAINTImprovedNeural}


- Process or data driven methods \cite{goncalves2020GenerationEvaluationSynthetic}
    - Process driven
        - Simulation models \cite{kowalczyk2022TaxonomyUseSynthetic}
    - Data driven
        - Data Augmentation \cite{kowalczyk2022TaxonomyUseSynthetic}
        - statistical distributions \cite{kowalczyk2022TaxonomyUseSynthetic}
        - ML/DL- based Approaches \cite{kowalczyk2022TaxonomyUseSynthetic}

%challanges
interdependencies between variables must be captures \cite{lederrey2022DATGANIntegratingExperta}
danger of overfitting and generelize relationships between columns which only are present in training data but not in unseen data \cite{lederrey2022DATGANIntegratingExperta}
syn data generation can help imbalance problem \cite{borisov2022DeepNeuralNetworks}



\cite{zhao2022CTABGANEnhancingTabular} identifies four complex distributions that can occur when working with real world tabular data.
underlying properties of tabular data: \cite{zhao2022CTABGANEnhancingTabular}
- Single Gaussian variables:
    Single mode Gaussian distributions are very common.
    The distribution of real data is close to a single mode Gaussian distribution

Mixed data type variables:\cite{zhao2022CTABGANEnhancingTabular}
    a variable can be a mix of these two types + missing values. The Mortgage variable from the Loan dataset is a good example of mixed variable
    According to the data description, a loan holder can either have no mortgage (0 value) or a mortgage (any positive value). 
    In appearance, this variable is not a categorical type due to the numeric nature of the data. 
    So all 4 SOTA algorithms treat this variable as continuous type without capturing the special meaning of the value zero. 
    Hence, all 4 algorithms generate a value around 0 instead of exact 0. And the negative values for Mortgage have no/wrong meaning in the real world

Long tail distributions:\cite{zhao2022CTABGANEnhancingTabular}
    real world data can have long tail distributions where most of the occurrences happen near the initial value of the distribution, and rare cases towards the end
    Real data clearly has 99\% of occurrences happening at the start of the range, 
    but the distribution extends until around 25000. 
    In comparison none of the synthetic data generators is able to learn and imitate this behavior.

Skewed multi-mode continuous variables:\cite{zhao2022CTABGANEnhancingTabular}
    The term multimode is extended from Variational Gaussian Mixtures (VGM)
    This is not a typical Gaussian distribution. There is an obvious peak
    with several other lower peaks
    This behavior is difficult to capture for the SOTA data generator


%-------------------------------------------------------------------------
\section{Deep Learning Architectures}
\label{ch:preliminaries-deepLearningArchitectures}

\subsection{Neural Networks}
\label{ch:preliminaries-deepLearningArchitectures-neuralNetworks}

- success of NN \& use of special architectures (rnn, cnn, etc.) in variety of domains \cite{borisov2022DeepNeuralNetworks}
- DL works well for homogeneous data \cite{borisov2022DeepNeuralNetworks}
% brief overview of neural networks (perceptron, multilayer perceptron)
% special neural networks 
% --> convolutional neural networks
% --> recurrent neural networks
% --> Residual neural networks
% --> Attention mechanism

\section{Generative Algorithms}
\label{ch:preliminaries-generativeAlgorithms}

\subsection{Autoencoders}
\label{ch:preliminaries-generativeAlgorithms-variationalAutoencoders}

from \cite{kingma2013AutoEncodingVariationalBayes}

Figure 1 from \cite{razghandi2022VariationalAutoencoderGenerativea}

% what are autoencoders
encoder transforms input into latent space which is reconstructed by decoder \cite{razghandi2022VariationalAutoencoderGenerativea}
--> suffer from "lack of regularity in latent space \cite{razghandi2022VariationalAutoencoderGenerativea}

% what are variational autoencoders
use the KL divergence and encode a gaussian distribution in latent space \cite{razghandi2022VariationalAutoencoderGenerativea}
% how do they work
% mathematical formulation
% Advantages / Problems / Challenges


\subsection{Generative Adversarial Networks}
\label{ch:preliminaries-generativeAlgorithms-generativeAdversarialNetworks}

% what are GAN's
\cite{goodfellow2020GenerativeAdversarialNetworks}
- have been used very sucessfully in many different domains \cite{li2022TTSGANTransformerbasedTimeSeries}
 

% how do they work
\cite{zhao2022CTABGANEnhancingTabular}:
- trained via a zero-sum min-max game 
- the discriminator tries to maximize the objective, while the generator tries to minimize it.
- mentor (D) providing feedback to a student (G) on the quality of his work


\cite{li2022TTSGANTransformerbasedTimeSeries}
- 2 NN (gen and dis)
- gen inp: rand vec of specified dimension; gen out: same dimension, as similar to real training data
- dis: binary classifier, distiguish real and generated data
--> play zero sum game against each other
--> trz to each nash equilibrium

% what improvements have been done to GAN's
- gans struggled with generating discrete variables \cite{torfi2020CorGANCorrelationCapturingConvolutionala}
- State of the art Gan approaches only focused on continouse and categorical types, overlooking mixed data types \cite{zhao2022CTABGANEnhancingTabular}

- conditional GAN \cite{mirza2014ConditionalGenerativeAdversarial}


% mathematical formulation
% Advantages / Problems / Challenges (Mode Collapse, etc.)
- remarkable performance generating syntehtic images and time series data \cite{mckeever2020SynthesisingTabularDatasets}
- struggle with mode collapse --> generate same sample \cite{torfi2020CorGANCorrelationCapturingConvolutionala}
- wasserstein loss helps against mode collapse \cite{frogner2015LearningWassersteinLoss} \cite{arjovsky2017WassersteinGenerativeAdversarial} and has been implemented in many gans (e.g. \cite{zhao2022CTABGANEnhancingTabular})
- gradient penalty \cite{gulrajani2017ImprovedTrainingWasserstein}


\subsection{Transformers}
\label{ch:preliminaries-generativeAlgorithms-transformers}
relies on multiple self-attention layers, surpasses other network architectures and shows properties of universal computation engine \cite{li2022TTSGANTransformerbasedTimeSeries}

have been very successfull on textual and visual data (//siehe quellen im paper) \cite{borisov2022DeepNeuralNetworks} and also been applied to tabular data \cite{padhi2021TabularTransformersModeling} \cite{gorishniy2022EmbeddingsNumericalFeatures}



% What are transformers
% How do they work
% In what context are they used (usually not for data synthesis)
% Advantages / Problems / Challenges

- TabNet is one of the first transformer-based models for tabular data \cite{borisov2022DeepNeuralNetworks}
- padhi2021TabularTransformersModeling

\subsection{Diffusion Probabilistic Models}
\label{ch:preliminaries-generativeAlgorithms-diffusionProbabilisticModels}

% What are diffusion probabilistic models
first paper \cite{sohl-dickstein2015DeepUnsupervisedLearning}
first famouse paper \cite{ho2020DenoisingDiffusionProbabilistic}
improvements> \cite{nichol2021ImprovedDenoisingDiffusion}
follow up: \cite{dhariwal2021DiffusionModelsBeat}

\cite{ho2022ClassifierFreeDiffusionGuidance}

\cite{rombach2022HighResolutionImageSynthesis}

% mathematical formulation
% How do they work
% In what context are they used (usually image synthesis)
% Advantages / Problems / Challenges


% for tabular data
\cite{zheng2022DiffusionModelsMissing} for missing data

\cite{kotelnikov2022TabDDPMModellingTabular} tabddpm
\cite{hoogeboom2021ArgmaxFlowsMultinomial} multinomial diffusion


\subsection{Diffusion Probabilistic Models for Tabular Data}
\label{ch:preliminaries-generativeAlgorithms-diffusionProbabilisticModelsTabularData}

% How can Diffusion Probabilistic Models be used for tabular data
% Challenges of using Diffusion Probabilistic Models for tabular data (mixed data types --> different noising process, etc.)


%-------------------------------------------------------------------------
\section{Evaluation of Synthetic Tabular Data}
\label{ch:preliminaries-evaluationOfSyntheticTabularData}

- there is no universal metric for data synthesis \cite{hernandez2022SyntheticDataGeneration}
- utility and information disclosure metric dimensions \cite{goncalves2020GenerationEvaluationSynthetic}
- structural similarity \cite{elemam2020SevenWaysEvaluate}

\subsection{Statistical Evaluation}
\label{ch:preliminaries-evaluationOfSyntheticTabularData-statisticalEvaluation}

\subsection{Machine Learning Efficiency}
\label{ch:preliminaries-evaluationOfSyntheticTabularData-machineLearningEfficiency}

\subsection{Privacy Evaluation}
\label{ch:preliminaries-evaluationOfSyntheticTabularData-privacyEvaluation}

\subsection{Additional Evaluation Methods}
\label{ch:preliminaries-evaluationOfSyntheticTabularData-otherMetrics}

% Bias and stability
% Domain Expertise

\subsection{Similarity Score}
\label{ch:preliminaries-evaluationOfSyntheticTabularData-similarityScore}
% https://www.researchgate.net/publication/344227988_On_the_Generation_and_Evaluation_of_Tabular_Data_using_GANs

% TOREAD: https://www.researchgate.net/publication/361949372_TabSynDex_A_Universal_Metric_for_Robust_Evaluation_of_Synthetic_Tabular_Data

%-------------------------------------------------------------------------




