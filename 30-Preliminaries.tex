\chapter{Preliminaries}
\label{ch:preliminaries}

%-------------------------------------------------------------------------
\section{Data Synthesis}
\label{ch:preliminaries-dataSynthesis}

\subsection{Synthetic Data}
\label{ch:preliminaries-dataSynthesis-syntheticData}
Synthetic data can be defined as "(...) artificially generated data, that are modelled on real data, with the same structure and properties as the original data (...)" \cite[p. 2]{kaloskampis2020SyntheticDataCivil} 
but without containing any actual specific information or entries of the actual real data. 
While first synthetic data approaches \cite{gelman1992InferenceIterativeSimulationa} focused on imputation techniques to generate synthetic data, the advent of deep learning has led to the topic gaining importance \cite{kowalczyk2022TaxonomyUseSynthetic, kaloskampis2020SyntheticDataCivil}
The importance of data is growing rapidly [TODO: QUELLE FINDEN] but accessing and collecting real data is usually expensive [TODO: QUELLE FINDEN] or simply not possible due to the sensitivity of the data (\eg medical records) \cite{esteban2017RealvaluedMedicalTimea} or due to regulatory restrictions, such as the \gls{gdpr} \cite{european_commission_regulation_2016}.
Synthetic data on the other hand is, compared to the collection of real data, cheap to generate \cite{leminh2021AirGenGANbasedSynthetica} and fulfils regulatory and privacy constraints \cite{zhao2022CTABGANEnhancingTabular}.

Hence, synthetic data could be used as an alternative to real data in various use-cases,
including but not limited to machine learning, software development or data sharing scenarios.
Access to data is still one of the biggest bottlenecks when developing machine learning or deep learning \glspl{model} \cite{fan2020RelationalDataSynthesisa}.
Synthetic data could be used to increase the data quality, by rebalancing skewed dataset \cite{zhao2022CTABGANEnhancingTabular} 
or increasing the dataset size as additional training data or in combination with the real data \cite{leminh2021AirGenGANbasedSynthetica, kim2021OCTGANNeuralODEbased}.
Synthetic data can also be used in situations where working with the real data is not possible, due to privacy or availability reasons [TODO: bessere quelle raussuchen]\cite{zhao2022CTABGANEnhancingTabular}.
In software development, high quality test data is crucial for development but challenging and time consuming to generate \cite{whiting2008CreatingRealisticScenariobased}.
Developers time to create such datasets is usually scarce and it might be the case that developers do not even have the permission to see the data, due to its sensitivity \cite{whiting2008CreatingRealisticScenariobased}.
A synthetic data generation \gls{model} would possibly allow developers to generate test data with predefined characteristics and without it being restricted in any form.
 




Upcoming new deep learning architectures have shown remarkable performance in the last decade %Gan transformer zititeren
they are able to generate synthetic data of many different data formats, such as images, video, music, ... %titieren






\subsection{Tabular Data}
\label{ch:preliminaries-dataSynthesis-tabularData}

Tabular data is the most one of the most common forms of structured structured data \cite{hernandez2022SyntheticDataGeneration} used to store, classify and share information \cite{pilaluisa2022ContextualWordEmbeddings}.
A table is made up of individual cell entries stored in rows and columns where rows can be seen as individual data points and columns are the different features \cite{borisov2022DeepNeuralNetworks, yoon2020VIMEExtendingSuccess}
This format is the most common way to maintain massive databases \cite{esmaeilpour2022BidiscriminatorGANTabular, yoon2020VIMEExtendingSuccess} 
and is crucial for applications that store heterogeneous information such as demographics, medical or financial information \cite{borisov2022DeepNeuralNetworks, yoon2020VIMEExtendingSuccess}.
Tabular data consists of multiple attribute types, such as categorical or continuous data types\cite{borisov2022DeepNeuralNetworks}.
While continuous values are of quantitative nature and stored in a numerical format, categorical values are made up by one value out of a limited set of values \cite{lederrey2022DATGANIntegratingExperta, lane2003IntroductionStatistics}.
Categorical attribute types can be further subdivided into binary, only two possible values, nominal, at least three possible values that do not follow any order and lastly ordinal with at least three entries where the values follow have an underlying order \cite{lederrey2022DATGANIntegratingExperta}.
This work adapts the formal definition of a table from \cite{xu2019ModelingTabularData}:

\begin{displayquote}
A table $T$ contains $N_{con}$ continuous columns and $N_{cat}$ categorical columns. [TODO: Definition ohne Discrete values]
\end{displayquote}

It is also possible that tabular data can contain other special data types like dates or timestamps which often contain information about the specific time a datapoint was recorded \cite{hernandez2022SyntheticDataGeneration}.
This kind of tabular data can be considered as dynamic tabular data, where individual records, \ie rows, can be dependent on each other, also known as a multivariate time series \cite{padhi2021TabularTransformersModeling}.
In static tabular data on the other hand the individual rows are independent from each other \cite{padhi2021TabularTransformersModeling}.
Hence, the order of rows and columns does not carry any meaning \cite{somepalli2021SAINTImprovedNeural}.
While the order of rows and columns does not matter, the individual values in one cell may vary well depend on values of another cell \cite{lederrey2022DATGANIntegratingExperta}.
An example for such an interdependency could occur in a demographics table, where an individual's legal status may depend on their age, 
for instance, a person under 18 years old is considered a minor and has different legal rights and responsibilities compared to an adult.

The authors of \cite{borisov2022DeepNeuralNetworks} identify four possible challenges when working with tabular data in a learning context.
The first identified challenge is the low quality of the data. Typical quality issues include missing values, noise in the data, extreme data points, data inconsistencies, class imbalance or high dimensionalities after preprocessing \cite{borisov2022DeepNeuralNetworks}[CONFIRM 1].
Secondly the missing irregular spatial dependencies of tabular data. Other common data formats like images or audio are homogeneous, 
meaning that they consists of only 1 feature type \cite{borisov2022DeepNeuralNetworks}.
Since tabular data consists of multiple features, made up of a mixture of categorical and numerical values, it is a heterogeneous data format with data points as rows and features as columns \cite{borisov2022DeepNeuralNetworks}.
This makes is especially challenging for neural networks to work with since the correlations between the features is weaker because they often do not have any form of spatial or semantic relationship like image or text data has \cite{borisov2022DeepNeuralNetworks, yoon2020VIMEExtendingSuccess}.
The third challenge is about the dependency on preprocessing. \cite{borisov2022DeepNeuralNetworks} highlights the importance of a preprocessing and explicit feature construction step that is necessary when working with tabular data in a deep learning context.
This preprocessing step is crucial since it does not only strongly influences the performance of deep learning \glspl{model} \cite{gorishniy2022EmbeddingsNumericalFeatures} 
it also introducing new challenges. Depending on the preprocessing strategy (\autoref{sec:preprocessing}) it is possible to create a very sparse feature matrix, create a synthetic ordinal ordering of a nominal variable or lose some information during the conversion of the data \cite{borisov2022DeepNeuralNetworks}.
The last and fourth challenge concerns the importance of a single feature. In homogeneous data multiple features need to change in order to change the class of the data. 
For heterogeneous tabular data a small change in one feature variable can already alter the class of the row. 
\cite{borisov2022DeepNeuralNetworks} illustrates this with the example of an image, where multiple pixels (\ie features) need to change in a coordinated manner in order to change the content (\ie class) of the image.
For tabular data a single change in a cell can change the prediction of a predictive \gls{model} \cite{borisov2022DeepNeuralNetworks}. 
Consider a \gls{model} that has to predict whether an individuals income is higher or lower than US\$ 50.000 per year \cite{Dua:2019} based on demographic information of that individual.
Switching an individuals "education" value from "Preschool" to "Doctorate" would likely cause the prediction to change from "<=50K" to ">50K".



\subsubsection{Tabular Data Preprocessing}
\label{sec:preprocessing}

Different data types can and should be processed into a meaningful format to be useful for deep learning \glspl{model} in different ways \cite{fan2020RelationalDataSynthesisa, lederrey2022DATGANIntegratingExperta}.
It is usually the first step before working with the data on any task \cite{izonin2022TwoStepDataNormalization}.
Data preprocessing itself consists of multiple different tasks: Data cleaning, data normalization, data transformation, data integration, missing value imputation and noise identification \cite{garcia2016BigDataPreprocessing}
While each of the preprocessing tasks is in itself important, 
\cite{fitkov-norris2012EvaluatingImpactCategorical} and \cite{gorishniy2022EmbeddingsNumericalFeatures} showed that a proper transformation of categorical and numerical entries respectively can have a significant influence on a deep learning \glspl{model} performance.
\cite{xu2019ModelingTabularData} showed the importance of normalization for synthetic tabular data generation.
Since the focus of the work is on tabular data and its synthesis, the following section will highlight the most important tabular data transformation and normalization approaches.

\subsubsubsection{Data Transformation}
\label{sec:dataTransformation}

\cite{borisov2022DeepNeuralNetworks} introduces a taxonomy for data transformation methods and subdivides the existing approaches into "Single-Dimensional Encodings" and "Multi-Dimensional Encodings".
The goal is to transform the different values column can take and transform them into a different (numeric) representation, such that it can be processed by a deep learning \gls{model}.

\textbf{Single-Dimensional Encodings:}
Single-Dimensional encoding techniques encode each cell independently \cite{borisov2022DeepNeuralNetworks}.
The following approaches are common techniques to encode a categorical column entry, usually in a text format, into a numerical format.
In ordinal- (or label-) encoding a simple mapping from each category to a numeric value occurs. 
While this introduces a synthetic ordering of potentially unordered categories, one-hot encoding overcomes this issue by introducing a new vector with the length of all possible values a categorical column can take.
All values in this vector are assign to zero except one entry that represents the category that should be encoded, which is set to one.
However, this approach can lead to high dimensionality feature vectors if the cardinality of the unique categories in categorical columns is large.
Binary encoding tries to reduce the dimensionality by setting the vector length to a maximum of $log(c)$ for $c$ unique categorical values in a column.
Each possible value is mapped to a number like in ordinal-encoding starting at 0 but the number is represented as a binary vector.
The leave-one-out encoding technique is an approach to encode a categorical column based on the target column in a machine learning scenario. 
A categorical entry is replaced by the mean of the target variable of all rows where the same category is present, excluding the target value of the to be encoded value.
Lastly, a hash-based approach is worth mentioning, where a deterministic hash function transforms each category into a numerical form \cite{borisov2022DeepNeuralNetworks}.

Numerical data, such as integers or floating point numbers, can often be used directly in deep learning \glspl{model} without undergoing a special encoding process. 
This is because deep learning algorithms are designed to handle numerical data and can learn patterns and relationships within the data without the need for additional encoding.
However, \cite{gorishniy2022EmbeddingsNumericalFeatures} has shown that in some cases, encoding numerical data can improve the performance of deep learning \glspl{model}. 
Encoding numerical data can be achieved through various methods such as normalization (see \autoref{sec:dataNormalization}), discretization, or using embeddings.

%Die beiden abschnitte auslagern in eigenen abschnitt? (GGF nur embeddings auslagern?)
Discretization techniques transform numerical features to categorical features, hence, quantitative data into qualitative data \cite{garcia2016BigDataPreprocessing}. 
\cite{dougherty1995SupervisedUnsupervisedDiscretization} gives an overview on classical discretization techniques, such has equal interval width binning, where the continues values are divided and assign to certain amount of bins.
A modern approach by researchers from NVIDIA \cite{dong2022GeneratingSyntheticData} invented a tokenizer specifically for tabular data with float numbers. 
This tokenizer converts float numbers into a sequence of token IDs \cite{dong2022GeneratingSyntheticData}.

In Embedding techniques values that should be encoded (\eg words or tabular cell entries) are mapped to a vector representation. 
This vector of real numbers tries to capture "semantic regularities in vector spaces" \cite[p. 2]{pilaluisa2022ContextualWordEmbeddings}.
The goal of embeddings is to create a vector space, in which semantically similar values are also numerically similar \cite{pilaluisa2022ContextualWordEmbeddings}.
It can be differentiated between static embeddings and contextualized embeddings. 
While the former embedding technique always provides the same numerical representation for an input value, the latter embedding technique changes the vector representation of value based on its surrounding context \cite{pilaluisa2022ContextualWordEmbeddings}.
This is especially important in the natural language processing domain, where contextual embeddings has led to state-of-the-art improvements \cite{pilaluisa2022ContextualWordEmbeddings}.
Homonyms or polysemic words like "bat" or "second" are words that carry multiple different meanings and their semantic meaning therefore changes with the context they are used in. [TODO: QUELLE]
Hence, the contextualized embedding vector of the word "second" in the context of time (\eg "it took me 3 seconds") should be different to the one where "second" is used in the context of a competition (\eg "he achieved the second place").
Contextualized embeddings have to be learned during some form of (pre-) training \cite{devlin2019BERTPretrainingDeep, iida2021TABBIEPretrainedRepresentations, deng2021TURLTableUnderstanding}. 
Static embeddings, such as Word2Vec \cite{mikolov2013DistributedRepresentationsWords} are learned as well. 
It is also possible to use the embedding layers to get a vector representation without learning, which can be seen as a "feature tokenization" \cite{zheng2022DiffusionModelsMissing, gorishniy2021RevisitingDeepLearning}.
However, semantic similarity in a vector space cannot be achieved without any learning, so this embedding technique is more similar to a discretization/tokenization technique.

[TODO: Insert example for single-dim encoding in tabular data]

\textbf{Multi-Dimensional Encodings:}
Multi-Dimensional encoding techniques focuses on encoding an entire record (or table-row) into another representation \cite{borisov2022DeepNeuralNetworks}.
The VIME framework \cite{yoon2020VIMEExtendingSuccess} uses a multilayer perceptron as an encoder that encodes a tabular input row into a latent representation, 
similar to the encoder of famous (variational-) auto-encoders \cite{kingma2013AutoEncodingVariationalBayes} architectures.
This latent representation is, like in embeddings, a learned representation learned through self-supervised learning tasks, feature vector estimation and mask vector estimation \cite{yoon2020VIMEExtendingSuccess}.
\cite{borisov2022DeepNeuralNetworks} also lists other multi-dimensional encoding techniques, such as the works of \cite{zhu2021ConvertingTabularData} and \cite{sun2019supertml}.
Both architectures transform tabular data into an image like format in order to make use of \glspl{cnn}.


%     2. multi-dimensional encoding 
%         - vime trains encoder learns a representations of cat and num features into homogeneous representation
%         - Supertml transforms tabular data into image format

\subsubsubsection{Data Normalization}
\label{sec:dataNormalization} 

Normalization techniques are applied to numerical columns and scales the values so that their distribution is adjusted \cite{garcia2016BigDataPreprocessing}.
This is especially important in tabular data, where numerical features are likely having a different scale, \eg a column "age" could have a range of 1 to 100 and a column income could have a range from 0 to 1 million [TODO: Quelle finden].
Normalization allows to scale each column to the same range (usually 0 to 1) while maintaining the overall distribution of each column \cite{izonin2022TwoStepDataNormalization}.
With this, the sensitivity of \gls{model} to large values is reduced and the generalizability is increased.
The most common normalization approaches are probably the StandardScaler and the MinMaxScaler.
The StandardScaler normalizes a feature value $x_i$ according to the mean and standard distribution of the feature column, so it follows a normal distribution \cite{garcia2016BigDataPreprocessing, izonin2022TwoStepDataNormalization}.
It can be defined as $x' = \frac{x_i-mean(x)}{std(x)}$, with $x'$ as the normalized $x_i$ value \cite{izonin2022TwoStepDataNormalization}.
The MinMaxScaler on the other hand makes use of the maximum and minimum of each feature column and scales the values accordingly.
It is defined as $x' = \frac{x_i - min(x)}{max(x) - min(x)}$ \cite{izonin2022TwoStepDataNormalization}.
While the StandardScaler is usually used for data that follows a gaussian normal distribution, MinMaxScaler is better suited for arbitrary distributions [TODO: QUELLE finden].
A more complex normalization technique addresses the problem, that numeric values in tabular data can follow distributions more complex than just a simple gaussian normal distribution \cite{zhao2022CTABGANEnhancingTabular, xu2019ModelingTabularData}.
\cite{xu2019ModelingTabularData} introduces mode-specific normalization which models complex distributions with multiple simple gaussian distributions.
\cite[p. 3-4]{xu2019ModelingTabularData} defines the mode-specific normalization technique in following way:

\begin{enumerate}
    \item For each continuos column $C_i$, estimate the number of modes $m_i$ by using a \gls{vgm} \cite{bishop2006PatternRecognitionMachine}. 
    For example, in \autoref{fig:mode-specific-normalization} the \gls{vgm} estimates $m_i=3$, $\eta_1$, $\eta_2$ and $\eta_3$. 
    The learned gaussian mixture is  
    $\mathbb{P}_{C_i}(c_{i,j})=\sum_{k=1}^{3}\mu_k\mathcal{N}(c_{i,j};\eta_k, \sigma_k)$
    with $\mu_k$ and $\sigma_k$ as weight and standard deviation of a mode respectively.
    \item For each $c_{i,j}$ the probability is calculated of $c_{i,j}$ coming from each mode. 
    In \autoref{fig:mode-specific-normalization}, $p_1$, $p_2$ and $p_3$ denote the probability densities, calculated as $p_k=\mu_k\mathcal{N}(c_{i,j};\eta_k;\sigma_k)$.
    \item Given $p_1$, $p_2$ and $p_3$ for $c_{i,j}$ sample one mode and use it to normalize the value. 
    In the example of \autoref{fig:mode-specific-normalization}, $p_3$ is most likely and is sampled.
    The mode selection for $c_{i,j}$ is represented as a one-hot vector $\beta_{i,j}=[0,0,1]$ indicating that the third mode has been sampled.
    The actual value $c_{i,j}$ is normalized as $\alpha_{i,j}=\frac{c_{i,j}-\eta_3}{4\sigma_3}$. 
\end{enumerate}

The normalized representation $r$ of $c_{i,j}$ will be the concatenation of $\alpha_{i,j}$ and $\beta_{i,j}$: 

$r=\alpha_{i,j}\oplus\beta_{i,j}$



% include image from paper
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{images/mode-normalization.png}
    \caption{An example of mode-specific normalization [TODO: COPYRIGHT?] \cite[Figure 1, p. 4]{xu2019ModelingTabularData}}
    \label{fig:mode-specific-normalization}
  \end{figure}


numerical:
- min-max normalization
- GMM-based normalization














\subsection{Synthetic Tabular Data Generation}

- two competing objectives in generating synthetic data: \cite{little2021GenerativeAdversarialNetworksa}
    1. high data utility \cite{little2021GenerativeAdversarialNetworksa}
    2. low disclosure risk \cite{little2021GenerativeAdversarialNetworksa}

--> eacg data point as "row in table", or "sample from unknown joint distribution"
order of rows and columns does not carry any meaning \cite{somepalli2021SAINTImprovedNeural}


- Process or data driven methods \cite{goncalves2020GenerationEvaluationSynthetic}
    - Process driven
        - Simulation models \cite{kowalczyk2022TaxonomyUseSynthetic}
    - Data driven
        - Data Augmentation \cite{kowalczyk2022TaxonomyUseSynthetic}
        - statistical distributions \cite{kowalczyk2022TaxonomyUseSynthetic}
        - ML/DL- based Approaches \cite{kowalczyk2022TaxonomyUseSynthetic}

%challanges
interdependencies between variables must be captures \cite{lederrey2022DATGANIntegratingExperta}
danger of overfitting and generelize relationships between columns which only are present in training data but not in unseen data \cite{lederrey2022DATGANIntegratingExperta}
syn data generation can help imbalance problem \cite{borisov2022DeepNeuralNetworks}



\cite{zhao2022CTABGANEnhancingTabular} identifies four complex distributions that can occur when working with real world tabular data.
underlying properties of tabular data: \cite{zhao2022CTABGANEnhancingTabular}
- Single Gaussian variables:
    Single mode Gaussian distributions are very common.
    The distribution of real data is close to a single mode Gaussian distribution

Mixed data type variables:\cite{zhao2022CTABGANEnhancingTabular}
    a variable can be a mix of these two types + missing values. The Mortgage variable from the Loan dataset is a good example of mixed variable
    According to the data description, a loan holder can either have no mortgage (0 value) or a mortgage (any positive value). 
    In appearance, this variable is not a categorical type due to the numeric nature of the data. 
    So all 4 SOTA algorithms treat this variable as continuous type without capturing the special meaning of the value zero. 
    Hence, all 4 algorithms generate a value around 0 instead of exact 0. And the negative values for Mortgage have no/wrong meaning in the real world

Long tail distributions:\cite{zhao2022CTABGANEnhancingTabular}
    real world data can have long tail distributions where most of the occurrences happen near the initial value of the distribution, and rare cases towards the end
    Real data clearly has 99\% of occurrences happening at the start of the range, 
    but the distribution extends until around 25000. 
    In comparison none of the synthetic data generators is able to learn and imitate this behavior.

Skewed multi-mode continuous variables:\cite{zhao2022CTABGANEnhancingTabular}
    The term multimode is extended from Variational Gaussian Mixtures (VGM)
    This is not a typical Gaussian distribution. There is an obvious peak
    with several other lower peaks
    This behavior is difficult to capture for the SOTA data generator


%-------------------------------------------------------------------------
\section{Deep Learning Architectures}
\label{ch:preliminaries-deepLearningArchitectures}

\subsection{Neural Networks}
\label{ch:preliminaries-deepLearningArchitectures-neuralNetworks}

- success of NN \& use of special architectures (rnn, cnn, etc.) in variety of domains \cite{borisov2022DeepNeuralNetworks}
- DL works well for homogeneous data \cite{borisov2022DeepNeuralNetworks}
% brief overview of neural networks (perceptron, multilayer perceptron)
% special neural networks 
% --> convolutional neural networks
% --> recurrent neural networks
% --> Residual neural networks
% --> Attention mechanism

\section{Generative Algorithms}
\label{ch:preliminaries-generativeAlgorithms}

\subsection{Autoencoders}
\label{ch:preliminaries-generativeAlgorithms-variationalAutoencoders}

from \cite{kingma2013AutoEncodingVariationalBayes}

Figure 1 from \cite{razghandi2022VariationalAutoencoderGenerativea}

% what are autoencoders
encoder transforms input into latent space which is reconstructed by decoder \cite{razghandi2022VariationalAutoencoderGenerativea}
--> suffer from "lack of regularity in latent space \cite{razghandi2022VariationalAutoencoderGenerativea}

% what are variational autoencoders
use the KL divergence and encode a gaussian distribution in latent space \cite{razghandi2022VariationalAutoencoderGenerativea}
% how do they work
% mathematical formulation
% Advantages / Problems / Challenges


\subsection{Generative Adversarial Networks}
\label{ch:preliminaries-generativeAlgorithms-generativeAdversarialNetworks}

% what are GAN's
\cite{goodfellow2020GenerativeAdversarialNetworks}
- have been used very sucessfully in many different domains \cite{li2022TTSGANTransformerbasedTimeSeries}
 

% how do they work
\cite{zhao2022CTABGANEnhancingTabular}:
- trained via a zero-sum min-max game 
- the discriminator tries to maximize the objective, while the generator tries to minimize it.
- mentor (D) providing feedback to a student (G) on the quality of his work


\cite{li2022TTSGANTransformerbasedTimeSeries}
- 2 NN (gen and dis)
- gen inp: rand vec of specified dimension; gen out: same dimension, as similar to real training data
- dis: binary classifier, distiguish real and generated data
--> play zero sum game against each other
--> trz to each nash equilibrium

% what improvements have been done to GAN's
- gans struggled with generating discrete variables \cite{torfi2020CorGANCorrelationCapturingConvolutionala}
- State of the art Gan approaches only focused on continouse and categorical types, overlooking mixed data types \cite{zhao2022CTABGANEnhancingTabular}

- conditional GAN \cite{mirza2014ConditionalGenerativeAdversarial}


% mathematical formulation
% Advantages / Problems / Challenges (Mode Collapse, etc.)
- remarkable performance generating syntehtic images and time series data \cite{mckeever2020SynthesisingTabularDatasets}
- struggle with mode collapse --> generate same sample \cite{torfi2020CorGANCorrelationCapturingConvolutionala}
- wasserstein loss helps against mode collapse \cite{frogner2015LearningWassersteinLoss} \cite{arjovsky2017WassersteinGenerativeAdversarial} and has been implemented in many gans (e.g. \cite{zhao2022CTABGANEnhancingTabular})
- gradient penalty \cite{gulrajani2017ImprovedTrainingWasserstein}


\subsection{Transformers}
\label{ch:preliminaries-generativeAlgorithms-transformers}
relies on multiple self-attention layers, surpasses other network architectures and shows properties of universal computation engine \cite{li2022TTSGANTransformerbasedTimeSeries}

have been very successfull on textual and visual data (//siehe quellen im paper) \cite{borisov2022DeepNeuralNetworks} and also been applied to tabular data \cite{padhi2021TabularTransformersModeling} \cite{gorishniy2022EmbeddingsNumericalFeatures}



% What are transformers
% How do they work
% In what context are they used (usually not for data synthesis)
% Advantages / Problems / Challenges

- TabNet is one of the first transformer-based models for tabular data \cite{borisov2022DeepNeuralNetworks}
- padhi2021TabularTransformersModeling

\subsection{Diffusion Probabilistic Models}
\label{ch:preliminaries-generativeAlgorithms-diffusionProbabilisticModels}

% What are diffusion probabilistic models
first paper \cite{sohl-dickstein2015DeepUnsupervisedLearning}
first famouse paper \cite{ho2020DenoisingDiffusionProbabilistic}
improvements> \cite{nichol2021ImprovedDenoisingDiffusion}
follow up: \cite{dhariwal2021DiffusionModelsBeat}

\cite{ho2022ClassifierFreeDiffusionGuidance}

\cite{rombach2022HighResolutionImageSynthesis}

% mathematical formulation
% How do they work
% In what context are they used (usually image synthesis)
% Advantages / Problems / Challenges


% for tabular data
\cite{zheng2022DiffusionModelsMissing} for missing data

\cite{kotelnikov2022TabDDPMModellingTabular} tabddpm
\cite{hoogeboom2021ArgmaxFlowsMultinomial} multinomial diffusion


\subsection{Diffusion Probabilistic Models for Tabular Data}
\label{ch:preliminaries-generativeAlgorithms-diffusionProbabilisticModelsTabularData}

% How can Diffusion Probabilistic Models be used for tabular data
% Challenges of using Diffusion Probabilistic Models for tabular data (mixed data types --> different noising process, etc.)


%-------------------------------------------------------------------------
\section{Evaluation of Synthetic Tabular Data}
\label{ch:preliminaries-evaluationOfSyntheticTabularData}

- there is no universal metric for data synthesis \cite{hernandez2022SyntheticDataGeneration}
- utility and information disclosure metric dimensions \cite{goncalves2020GenerationEvaluationSynthetic}
- structural similarity \cite{elemam2020SevenWaysEvaluate}

\subsection{Statistical Evaluation}
\label{ch:preliminaries-evaluationOfSyntheticTabularData-statisticalEvaluation}

\subsection{Machine Learning Efficiency}
\label{ch:preliminaries-evaluationOfSyntheticTabularData-machineLearningEfficiency}

\subsection{Privacy Evaluation}
\label{ch:preliminaries-evaluationOfSyntheticTabularData-privacyEvaluation}

\subsection{Additional Evaluation Methods}
\label{ch:preliminaries-evaluationOfSyntheticTabularData-otherMetrics}

% Bias and stability
% Domain Expertise

\subsection{Similarity Score}
\label{ch:preliminaries-evaluationOfSyntheticTabularData-similarityScore}
% https://www.researchgate.net/publication/344227988_On_the_Generation_and_Evaluation_of_Tabular_Data_using_GANs

% TOREAD: https://www.researchgate.net/publication/361949372_TabSynDex_A_Universal_Metric_for_Robust_Evaluation_of_Synthetic_Tabular_Data

%-------------------------------------------------------------------------




